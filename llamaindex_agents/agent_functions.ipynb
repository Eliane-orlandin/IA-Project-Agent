{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/elianeorlandin/Documents/Desenvolvimento/IA-Project-Agent/llamaindex_agents/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import Settings\n",
    "from llama_index.llms.groq import Groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "llm = Groq(model=\"llama-3.3-70b-versatile\", api_key=os.environ.get(\"GROQ_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcular_imposto_renda(rendimento:float) -> str:\n",
    "    \"\"\" \n",
    "    Calcula o imposto de renda com base no rendimento anual.\n",
    "    Args:\n",
    "        rendimento (float): Rendimento anual do contribuinte.\n",
    "    Returns:\n",
    "        str: O valor do imposto devido com base no rendimento.\n",
    "    \"\"\"\n",
    "    if rendimento <= 2000:\n",
    "        return \"Isento de imposto de renda.\"\n",
    "    elif 2000 < rendimento <= 5000:\n",
    "        imposto = (rendimento - 2000) * 0.10\n",
    "        return f\"Imposto devido √© de: R$ {imposto:.2f}, base em um rendimento de R$ {rendimento:.2f}.\"\n",
    "    elif 5000 < rendimento <= 10000:\n",
    "        imposto = (rendimento - 5000) * 0.15 + 300\n",
    "        return f\"Imposto devido √© de: R$ {imposto:.2f}, base em um rendimento de R$ {rendimento:.2f}.\"\n",
    "    else:\n",
    "        imposto = (rendimento - 10000) * 0.20 + 1050\n",
    "        return f\"Imposto devido √© de: R$ {imposto:.2f}, base em um rendimento de R$ {rendimento:.2f}.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convertendo Fun√ß√£o em Ferramenta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import FunctionTool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ferramenta_imposto_renda = FunctionTool.from_defaults(\n",
    "    fn=calcular_imposto_renda,\n",
    "    name=\"Calcular Imposto de Renda\",\n",
    "    description=(\n",
    "    \"Calcula o imposto de renda com base no rendimento anual do contribuinte.\"\n",
    "    \"Argumento: redimento (float).\"\n",
    "    \"Retorna o valor do imposto devido de acordo com faixas de rendimento.\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent import FunctionCallingAgentWorker "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_worker_imposto = FunctionCallingAgentWorker.from_tools(\n",
    "    tools=[ferramenta_imposto_renda],\n",
    "    verbose=True,\n",
    "    allow_parallel_tool_calls=True,\n",
    "    llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent import AgentRunner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_imposto= AgentRunner(agent_worker_imposto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: \n",
      "    Qual √© o imposto de renda devido por uma pessoa com rendimento anual de R$ 7500?\n",
      "\n",
      "=== Calling Function ===\n",
      "Calling function: Calcular Imposto de Renda with args: {\"rendimento\": 7500}\n",
      "=== Function Output ===\n",
      "Imposto devido √© de: R$ 675.00, base em um rendimento de R$ 7500.00.\n",
      "=== LLM Response ===\n",
      "O imposto de renda devido por uma pessoa com rendimento anual de R$ 7500 √© de R$ 675,00.\n"
     ]
    }
   ],
   "source": [
    "response = agent_imposto.chat(\"\"\"\n",
    "    Qual √© o imposto de renda devido por uma pessoa com rendimento anual de R$ 7500?\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv \n",
    "\n",
    "def consulta_artigos(titulo: str) -> str:\n",
    "    \"\"\" Consulta artigos na base de dados arXiv e retorna resultados formatados.\"\"\"\n",
    "    busca = arxiv.Search(\n",
    "        query=titulo,\n",
    "        max_results=5,\n",
    "        sort_by=arxiv.SortCriterion.Relevance\n",
    "    )\n",
    "\n",
    "    resultados = [\n",
    "        f\"T√≠tulo: {artigo.title}\\n\"\n",
    "        f\"Categoria: {artigo.primary_category}\\n\"\n",
    "        f\"Link: {artigo.entry_id}\\n\"\n",
    "        for artigo in busca.results()\n",
    "    ]\n",
    "\n",
    "    return \"\\n\\n\".join(resultados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "consulta_artigos_tool = FunctionTool.from_defaults(fn=consulta_artigos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
    "    [ferramenta_imposto_renda, consulta_artigos_tool],\n",
    "    verbose=True,\n",
    "    allow_parallel_tool_calls=False,\n",
    "    llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Me retorne artigos sobre LangChain na eeduca√ß√£o\n",
      "=== Calling Function ===\n",
      "Calling function: consulta_artigos with args: {\"titulo\": \"LangChain na educa\\u00e7\\u00e3o\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5z/bv195b9d0f11m9c66rw80j3r0000gp/T/ipykernel_10203/3605877752.py:15: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead\n",
      "  for artigo in busca.results()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Function Output ===\n",
      "T√≠tulo: Development and Testing of Retrieval Augmented Generation in Large Language Models -- A Case Study Report\n",
      "Categoria: cs.CL\n",
      "Link: http://arxiv.org/abs/2402.01733v1\n",
      "\n",
      "\n",
      "T√≠tulo: From Prompt Injections to SQL Injection Attacks: How Protected is Your LLM-Integrated Web Application?\n",
      "Categoria: cs.CR\n",
      "Link: http://arxiv.org/abs/2308.01990v4\n",
      "\n",
      "\n",
      "T√≠tulo: Automating Customer Service using LangChain: Building custom open-source GPT Chatbot for organizations\n",
      "Categoria: cs.CL\n",
      "Link: http://arxiv.org/abs/2310.05421v1\n",
      "\n",
      "\n",
      "T√≠tulo: Poisoned LangChain: Jailbreak LLMs by LangChain\n",
      "Categoria: cs.CL\n",
      "Link: http://arxiv.org/abs/2406.18122v1\n",
      "\n",
      "\n",
      "T√≠tulo: Breast Ultrasound Report Generation using LangChain\n",
      "Categoria: eess.IV\n",
      "Link: http://arxiv.org/abs/2312.03013v1\n",
      "\n",
      "=== Calling Function ===\n",
      "Calling function: consulta_artigos with args: {\"titulo\": \"LangChain e educa\\u00e7\\u00e3o\"}\n",
      "=== Function Output ===\n",
      "T√≠tulo: From Prompt Injections to SQL Injection Attacks: How Protected is Your LLM-Integrated Web Application?\n",
      "Categoria: cs.CR\n",
      "Link: http://arxiv.org/abs/2308.01990v4\n",
      "\n",
      "\n",
      "T√≠tulo: Automating Customer Service using LangChain: Building custom open-source GPT Chatbot for organizations\n",
      "Categoria: cs.CL\n",
      "Link: http://arxiv.org/abs/2310.05421v1\n",
      "\n",
      "\n",
      "T√≠tulo: Poisoned LangChain: Jailbreak LLMs by LangChain\n",
      "Categoria: cs.CL\n",
      "Link: http://arxiv.org/abs/2406.18122v1\n",
      "\n",
      "\n",
      "T√≠tulo: Breast Ultrasound Report Generation using LangChain\n",
      "Categoria: eess.IV\n",
      "Link: http://arxiv.org/abs/2312.03013v1\n",
      "\n",
      "\n",
      "T√≠tulo: Revolutionizing Mental Health Care through LangChain: A Journey with a Large Language Model\n",
      "Categoria: cs.HC\n",
      "Link: http://arxiv.org/abs/2403.05568v1\n",
      "\n",
      "=== Calling Function ===\n",
      "Calling function: consulta_artigos with args: {\"titulo\": \"LangChain aplicado \\u00e0 educa\\u00e7\\u00e3o\"}\n",
      "=== Function Output ===\n",
      "T√≠tulo: From Prompt Injections to SQL Injection Attacks: How Protected is Your LLM-Integrated Web Application?\n",
      "Categoria: cs.CR\n",
      "Link: http://arxiv.org/abs/2308.01990v4\n",
      "\n",
      "\n",
      "T√≠tulo: M√©todo de elementos finitos aplicado a las ecuaciones de Stokes y de Advecci√≥n-Difusi√≥n\n",
      "Categoria: math.NA\n",
      "Link: http://arxiv.org/abs/1401.7619v1\n",
      "\n",
      "\n",
      "T√≠tulo: M√©todo de Monte Carlo aplicado ao C√°lculo Fracion√°rio\n",
      "Categoria: math.NA\n",
      "Link: http://arxiv.org/abs/2110.08129v1\n",
      "\n",
      "\n",
      "T√≠tulo: Automating Customer Service using LangChain: Building custom open-source GPT Chatbot for organizations\n",
      "Categoria: cs.CL\n",
      "Link: http://arxiv.org/abs/2310.05421v1\n",
      "\n",
      "\n",
      "T√≠tulo: Poisoned LangChain: Jailbreak LLMs by LangChain\n",
      "Categoria: cs.CL\n",
      "Link: http://arxiv.org/abs/2406.18122v1\n",
      "\n",
      "=== LLM Response ===\n",
      "Infelizmente, n√£o foi poss√≠vel encontrar artigos sobre LangChain aplicado √† educa√ß√£o. No entanto, voc√™ pode tentar procurar em outras bases de dados ou realizar uma busca mais espec√≠fica para encontrar resultados mais relevantes. Al√©m disso, √© importante notar que a pesquisa sobre LangChain e sua aplica√ß√£o em diferentes √°reas, incluindo a educa√ß√£o, √© um campo em constante evolu√ß√£o, e novos estudos e artigos podem ser publicados a qualquer momento.\n"
     ]
    }
   ],
   "source": [
    "agent = AgentRunner(agent_worker)\n",
    "response = agent.chat(\"Me retorne artigos sobre LangChain na eeduca√ß√£o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usando o Tavily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tavily_key = os.environ.get(\"TAVILY_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.tools.tavily_research import TavilyToolSpec\n",
    "\n",
    "tavily_tool = TavilyToolSpec(\n",
    "    api_key=tavily_key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search\n"
     ]
    }
   ],
   "source": [
    "tavily_tool_list = tavily_tool.to_tool_list()\n",
    "for tool in tavily_tool_list:\n",
    "    print(tool.metadata.name)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id_='29cb1e12-0732-4fcc-ba58-101ad8f4f339', embedding=None, metadata={'url': 'https://community.revelo.com.br/faca-perguntas-ao-seu-pdf-usando-langchain-llama-2-e-python/'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Durante 2023, a Meta anunciou o LLaMA 2 (de agora em diante vou cham√°-lo simplesmente de Llama 2), um LLM de c√≥digo aberto que √© a evolu√ß√£o de seu modelo anterior (LLaMA 1), que pode ser usado para criar aplica√ß√µes para fins comerciais. Para carregar o modelo Llama 2, como aconteceu com o carregamento de PDF e Pinecode, LangChain tamb√©m nos fornece uma interface (que f√°cil!). Isto n√£o significa que devolva dois par√°grafos ou duas palavras, mas tem a ver com os textos que o modelo considera significativos para fornecer uma resposta: e um documento pode conter v√°rios par√°grafos. O que se v√™ neste artigo √© apenas um vislumbre das capacidades do LangChain, j√° que possui muitas outras integra√ß√µes e, al√©m disso, permite que seja utilizado ‚Äîatrav√©s dos plugins‚Äî com outros modelos como ChatGPT.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tavily_tool.search(\"Me rteorne artigos cient√≠ficos sobre LangChain\", max_results=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defini√ß√£o de ferramentas externas e integra√ß√£o com o agente  \n",
    "Criamos uma ferramenta utilizando a API do Tavily para realizar buscas sobre um determinado t√≥pico.  \n",
    "Essa ferramenta ser√° incorporada ao agente para permitir pesquisas contextuais.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import FunctionTool\n",
    "tavily_tool_function = FunctionTool.from_defaults(\n",
    "    fn=tavily_tool.search,\n",
    "    name=\"Tavily Search\",\n",
    "    description=(\n",
    "        \"Busca artigos com Tavily sobre um determinado t√≥pico\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
    "    tools=[tavily_tool_function],\n",
    "    verbose=True,\n",
    "    allow_parallel_tool_calls=False,\n",
    "    llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = AgentRunner(agent_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Me retorne artigos sobre LLM e LangChain\n",
      "=== Calling Function ===\n",
      "Calling function: Tavily Search with args: {\"max_results\": 6, \"query\": \"LLM e LangChain\"}\n",
      "=== Function Output ===\n",
      "[Document(id_='8c34e89a-924c-43fc-bd19-0d50e14e8d26', embedding=None, metadata={'url': 'https://www.techtarget.com/searchenterpriseai/tip/How-to-use-LangChain-for-LLM-application-development'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='[LangChain](https://www.techtarget.com/searchenterpriseai/definition/LangChain), an open source framework for building AI applications, has become a de facto standard for working with LLMs and integrating [APIs](https://www.techtarget.com/searchapparchitecture/definition/application-program-interface-API). The tool serves as a critical intermediary, enabling a targeted LLM to interface with traditional software. [...] LangChain enables developers to integrate AI models with standard IT components such as software utilities, APIs and databases. Within LangChain, developers use a combination of prompts, tools and chains to manage LLMs and create desired AI interactions.\\n\\n### Prompts\\n\\nAll processes in LangChain revolve around [prompts](https://www.techtarget.com/searchenterpriseai/definition/AI-prompt), which initiate the tasks that AI applications rely on. [...] Using LangChain with Spark and Kafka\\n------------------------------------\\n\\nLangChain excels at managing LLM workflows and integrating language models with APIs, tools and software utilities. But AI applications often require more than just model orchestration.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='0fc8abcf-f41b-4ba2-bc70-61fb6a41950a', embedding=None, metadata={'url': 'https://vitiya99.medium.com/building-scalable-llm-applications-with-langchain-and-vector-databases-5541c0ebee0e'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Large Language Models (LLMs) are transforming how we interact with information and build intelligent applications. However, effectively leveraging their power for real-world scenarios often requires extending their capabilities beyond simple prompt-response interactions. This is where LangChain and vector databases come into play. LangChain provides a streamlined framework for developing LLM-powered applications, while vector databases facilitate efficient semantic search, enabling LLMs to [...] LangChain simplifies the complexities of integrating LLMs into applications by providing abstractions for common tasks like prompt management, chain execution, and memory management. Vector databases, such as Pinecone, Weaviate, and Faiss, store embeddings (vector representations of data) generated by models like Sentence Transformers or OpenAI‚Äôs embeddings API. This allows for similarity search, enabling the retrieval of contextually relevant information based on semantic meaning rather than [...] keyword matching. The combination of LangChain and vector databases empowers developers to build sophisticated LLM applications that can access, process, and reason over large amounts of unstructured data.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='c00ed611-1b03-4cf4-8dc8-77a9dd241cee', embedding=None, metadata={'url': 'https://scalexi.medium.com/understanding-the-differences-between-llm-chains-and-llm-agent-executors-in-langchain-3f3cf402442f'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=\"**LangChain** has emerged as a robust framework for building applications powered by large language models (LLMs). Two fundamental concepts within LangChain are **LLM Chains** and **LLM Agent Executors**, both of which leverage tools to enhance the capabilities of LLMs. While they may seem similar at first glance, understanding their differences is crucial for developers aiming to harness LangChain's full potential. [...] Both **LLM Chains** and **LLM Agent Executors** offer powerful ways to structure and execute tasks using LangChain, but they are designed for different use cases. Understanding the key differences is essential for choosing the right approach.\", path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='88498561-b8f8-4e79-b22d-aca3e7dd2392', embedding=None, metadata={'url': 'https://python.langchain.com/docs/integrations/llms/'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Unless you are specifically using more advanced prompting techniques, you are probably looking for [this page instead](https://python.langchain.com/docs/integrations/chat/).\\n\\n[LLMs](https://python.langchain.com/docs/concepts/text_llms/) are language models that take a string as input and return a string as output.\\n\\ninfo [...] [![Image 2: Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/integrations/llms/index.mdx)\\n\\nLLMs\\n====\\n\\ncaution\\n\\nYou are currently on a page documenting the use of [text completion models](https://python.langchain.com/docs/concepts/text_llms/). Many of the latest and most popular models are [chat completion models](https://python.langchain.com/docs/concepts/chat_models/). [...] *   [Kinetica](https://python.langchain.com/docs/integrations/chat/kinetica/)\\n        *   [Konko](https://python.langchain.com/docs/integrations/chat/konko/)\\n        *   [LiteLLM](https://python.langchain.com/docs/integrations/chat/litellm/)\\n        *   [Llama 2 Chat](https://python.langchain.com/docs/integrations/chat/llama2_chat/)\\n        *   [Llama API](https://python.langchain.com/docs/integrations/chat/llama_api/)', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='999aa9f9-ca32-4830-bc52-7044d316cbce', embedding=None, metadata={'url': 'https://python.langchain.com/docs/tutorials/llm_chain/'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=\"In this quickstart we'll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it's just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!\\n\\nAfter reading this tutorial, you'll have a high level overview of: [...] Many of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with [LangSmith](https://smith.langchain.com/).\\n\\nAfter you sign up at the link above, make sure to set your environment variables to start logging traces: [...] [![Image 1: ü¶úÔ∏èüîó LangChain](https://python.langchain.com/img/brand/wordmark.png)](https://python.langchain.com/)[Integrations](https://python.langchain.com/docs/integrations/providers/)[API Reference](https://python.langchain.com/api_reference/)\\n\\n[More](https://python.langchain.com/docs/tutorials/llm_chain/#)\", path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='d90d9fb5-b2d6-4d4e-96ec-7e303ab941fa', embedding=None, metadata={'url': 'https://vstorm.co/the-power-of-langchain-in-llm-based-applications/'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='LangChain, a framework specifically designed for Large Language Model (LLM) applications, has emerged as a major tool in enhancing the capabilities of natural', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}')]\n",
      "=== LLM Response ===\n",
      "Os artigos sobre LLM e LangChain abordam a integra√ß√£o de modelos de linguagem grandes (LLMs) com a framework LangChain para desenvolver aplica√ß√µes de intelig√™ncia artificial (IA) mais avan√ßadas. A LangChain fornece uma estrutura para gerenciar workflows de LLMs e integr√°-los com APIs, ferramentas e utilit√°rios de software, permitindo que os desenvolvedores criem aplica√ß√µes de IA mais complexas e escal√°veis.\n",
      "\n",
      "Os artigos destacam a import√¢ncia da LangChain para simplificar a integra√ß√£o de LLMs em aplica√ß√µes, fornecendo abstra√ß√µes para tarefas comuns como gerenciamento de prompts, execu√ß√£o de chains e gerenciamento de mem√≥ria. Al√©m disso, a combina√ß√£o da LangChain com bancos de dados de vetores permite a realiza√ß√£o de buscas sem√¢nticas eficientes, permitindo que os LLMs acessem e processem grandes quantidades de dados n√£o estruturados.\n",
      "\n",
      "Os artigos tamb√©m discutem a diferen√ßa entre LLM Chains e LLM Agent Executors, dois conceitos fundamentais na LangChain, e como eles podem ser usados para estruturar e executar tarefas em aplica√ß√µes de IA. Al√©m disso, os artigos fornecem exemplos de como a LangChain pode ser usada para desenvolver aplica√ß√µes de IA, como tradu√ß√£o de texto e busca sem√¢ntica.\n",
      "\n",
      "Em resumo, os artigos sobre LLM e LangChain destacam a import√¢ncia da integra√ß√£o de modelos de linguagem grandes com a framework LangChain para desenvolver aplica√ß√µes de intelig√™ncia artificial mais avan√ßadas e escal√°veis. A LangChain fornece uma estrutura para gerenciar workflows de LLMs e integr√°-los com APIs, ferramentas e utilit√°rios de software, permitindo que os desenvolvedores criem aplica√ß√µes de IA mais complexas e eficientes.\n"
     ]
    }
   ],
   "source": [
    "response = agent.chat(\"Me retorne artigos sobre LLM e LangChain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Os artigos sobre LLM e LangChain abordam a integra√ß√£o de modelos de linguagem grandes (LLMs) com a framework LangChain para desenvolver aplica√ß√µes de intelig√™ncia artificial (IA) mais avan√ßadas. A LangChain fornece uma estrutura para gerenciar workflows de LLMs e integr√°-los com APIs, ferramentas e utilit√°rios de software, permitindo que os desenvolvedores criem aplica√ß√µes de IA mais complexas e escal√°veis.\n",
      "\n",
      "Os artigos destacam a import√¢ncia da LangChain para simplificar a integra√ß√£o de LLMs em aplica√ß√µes, fornecendo abstra√ß√µes para tarefas comuns como gerenciamento de prompts, execu√ß√£o de chains e gerenciamento de mem√≥ria. Al√©m disso, a combina√ß√£o da LangChain com bancos de dados de vetores permite a realiza√ß√£o de buscas sem√¢nticas eficientes, permitindo que os LLMs acessem e processem grandes quantidades de dados n√£o estruturados.\n",
      "\n",
      "Os artigos tamb√©m discutem a diferen√ßa entre LLM Chains e LLM Agent Executors, dois conceitos fundamentais na LangChain, e como eles podem ser usados para estruturar e executar tarefas em aplica√ß√µes de IA. Al√©m disso, os artigos fornecem exemplos de como a LangChain pode ser usada para desenvolver aplica√ß√µes de IA, como tradu√ß√£o de texto e busca sem√¢ntica.\n",
      "\n",
      "Em resumo, os artigos sobre LLM e LangChain destacam a import√¢ncia da integra√ß√£o de modelos de linguagem grandes com a framework LangChain para desenvolver aplica√ß√µes de intelig√™ncia artificial mais avan√ßadas e escal√°veis. A LangChain fornece uma estrutura para gerenciar workflows de LLMs e integr√°-los com APIs, ferramentas e utilit√°rios de software, permitindo que os desenvolvedores criem aplica√ß√µes de IA mais complexas e eficientes.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 16 0 (offset 0)\n",
      "Ignoring wrong pointing object 18 0 (offset 0)\n",
      "Ignoring wrong pointing object 20 0 (offset 0)\n",
      "Ignoring wrong pointing object 22 0 (offset 0)\n",
      "Ignoring wrong pointing object 42 0 (offset 0)\n",
      "Ignoring wrong pointing object 50 0 (offset 0)\n",
      "Ignoring wrong pointing object 52 0 (offset 0)\n",
      "Ignoring wrong pointing object 54 0 (offset 0)\n",
      "Ignoring wrong pointing object 56 0 (offset 0)\n",
      "Ignoring wrong pointing object 58 0 (offset 0)\n",
      "Ignoring wrong pointing object 70 0 (offset 0)\n",
      "Ignoring wrong pointing object 72 0 (offset 0)\n",
      "Ignoring wrong pointing object 89 0 (offset 0)\n",
      "Ignoring wrong pointing object 91 0 (offset 0)\n",
      "Ignoring wrong pointing object 103 0 (offset 0)\n",
      "Ignoring wrong pointing object 108 0 (offset 0)\n",
      "Ignoring wrong pointing object 149 0 (offset 0)\n",
      "Ignoring wrong pointing object 155 0 (offset 0)\n",
      "Ignoring wrong pointing object 158 0 (offset 0)\n",
      "Ignoring wrong pointing object 160 0 (offset 0)\n",
      "Ignoring wrong pointing object 163 0 (offset 0)\n",
      "Ignoring wrong pointing object 165 0 (offset 0)\n"
     ]
    }
   ],
   "source": [
    "url = \"files/LLM.pdf\"\n",
    "artigo = SimpleDirectoryReader(input_files=[url]).load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"files/LLM_2.pdf\"\n",
    "tutorial = SimpleDirectoryReader(input_files=[url]).load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gerar os Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "Settings.embed_model = HuggingFaceEmbedding(\n",
    "    model_name= \"inTfloat/multilingual-e5-large\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "artigo_index = VectorStoreIndex.from_documents(artigo)\n",
    "tutorial_index = VectorStoreIndex.from_documents(tutorial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "artigo_index.storage_context.persist(persist_dir=\"artigo\")\n",
    "tutorial_index.storage_context.persist(persist_dir=\"tutorial\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Engine de Busca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import StorageContext, load_index_from_storage    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_context = StorageContext.from_defaults(\n",
    "    persist_dir=\"artigo\"\n",
    ")\n",
    "artigo_index = load_index_from_storage(storage_context)\n",
    "\n",
    "storage_context = StorageContext.from_defaults(\n",
    "    persist_dir=\"tutorial\"\n",
    ")\n",
    "tutorial_index = load_index_from_storage(storage_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "artigo_engine = artigo_index.as_query_engine(similary_top_k=3, llm=llm)\n",
    "tutorial_engine = tutorial_index.as_query_engine(similary_top_k=3, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import QueryEngineTool, ToolMetadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine_tools = [\n",
    "    QueryEngineTool(\n",
    "        query_engine=artigo_engine,\n",
    "        metadata=ToolMetadata(\n",
    "            name=\"artigo_engine\",\n",
    "            description=(\n",
    "                \"Fornece informa√ß√µe4s sobre LLM e LangChain.\"\n",
    "                \"Use uma pergunta detalhada em texto simples como entrada para a ferramenta\"\n",
    "            )\n",
    "        )\n",
    "    ),\n",
    "    QueryEngineTool(\n",
    "        query_engine=tutorial_engine,\n",
    "        metadata=ToolMetadata(\n",
    "            name=\"tutorial_engine\",\n",
    "            description=(\n",
    "                \"Fornece informa√ß√µe4s sobre casos de uso e aplica√ß√µes em LLMs.\"\n",
    "                \"Use uma pergunta detalhada em texto simples como entrada para a ferramenta\"\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
    "    query_engine_tools,\n",
    "    verbose=True,\n",
    "    allow_parallel_tool_calls=True,\n",
    "    llm=llm\n",
    ")\n",
    "agent_document = AgentRunner(agent_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Quais as principais aplica√ß√µes posso construir com LLMs e LangChain?\n",
      "=== Calling Function ===\n",
      "Calling function: artigo_engine with args: {\"input\": \"Quais as principais aplica\\u00e7\\u00f5es posso construir com LLMs e LangChain?\"}\n",
      "=== Function Output ===\n",
      "Com LLMs, voc√™ pode desenvolver aplicativos prontos para produ√ß√£o, como modelos de linguagem personalizados para atender √†s necessidades espec√≠ficas do seu dom√≠nio. Al√©m disso, voc√™ pode ajustar os modelos de c√≥digo aberto para melhorar o desempenho em seu dom√≠nio espec√≠fico. Isso inclui a capacidade de treinar os modelos com seus dados espec√≠ficos, permitindo uma maior precis√£o e controle sobre os resultados. Com a combina√ß√£o de LLMs e ferramentas como a Databricks, voc√™ pode criar solu√ß√µes personalizadas para suas necessidades, desde a cria√ß√£o de modelos de linguagem at√© a implementa√ß√£o de solu√ß√µes de dados.\n",
      "=== Calling Function ===\n",
      "Calling function: tutorial_engine with args: {\"input\": \"Quais as principais aplica\\u00e7\\u00f5es posso construir com LLMs e LangChain?\"}\n",
      "=== Function Output ===\n",
      "As principais aplica√ß√µes que podem ser constru√≠das com LLMs incluem:\n",
      "\n",
      "1. Cria√ß√£o e aprimoramento de conte√∫do, como:\n",
      " * Gera√ß√£o de conte√∫do: produ√ß√£o autom√°tica de texto\n",
      " * Assist√™ncia na reda√ß√£o: corre√ß√£o ortogr√°fica, de estilo e de conte√∫do\n",
      " * Tradu√ß√£o autom√°tica: convers√£o de texto de um idioma para outro\n",
      " * Resumo de textos: redu√ß√£o de documentos longos em resumos\n",
      " * Planejamento e roteiro de conte√∫do: estrutura√ß√£o do conte√∫do\n",
      " * Brainstorming: propostas criativas para projetos, nomes, conceitos, etc.\n",
      " * Programa√ß√£o: cria√ß√£o de c√≥digo de programa√ß√£o a partir de linguagem natural\n",
      "\n",
      "2. An√°lise e organiza√ß√£o de informa√ß√µes, como:\n",
      " * An√°lise de sentimento: avalia√ß√£o de emo√ß√µes e opini√µes em textos\n",
      " * Extra√ß√£o de informa√ß√µes: extra√ß√£o de dados espec√≠ficos de documentos grandes\n",
      " * Classifica√ß√£o de textos: organiza√ß√£o de textos em categorias ou temas espec√≠ficos\n",
      " * Revis√£o t√©cnica: assist√™ncia na revis√£o de documentos especializados\n",
      "\n",
      "3. Intera√ß√£o e automa√ß√£o, como:\n",
      " * Chatbots: simula√ß√£o de conversas sobre t√≥picos gerais ou espec√≠ficos\n",
      " * Perguntas e respostas: gera√ß√£o de respostas a perguntas com base em um corpus\n",
      "\n",
      "Al√©m disso, com o surgimento dos LLMs multimodais, outras aplica√ß√µes est√£o come√ßando a surgir, como a gera√ß√£o de conte√∫do audiovisual, a interpreta√ß√£o de dados de imagens, a tradu√ß√£o de conte√∫do multim√≠dia ou a cria√ß√£o de experi√™ncias interativas ricas.\n",
      "=== Calling Function ===\n",
      "Calling function: artigo_engine with args: {\"input\": \"Quais as principais aplica\\u00e7\\u00f5es posso construir com LLMs e LangChain?\"}\n",
      "=== Function Output ===\n",
      "Com LLMs, voc√™ pode desenvolver aplicativos prontos para produ√ß√£o, como modelos de linguagem personalizados para atender √†s necessidades espec√≠ficas do seu dom√≠nio. Al√©m disso, voc√™ pode ajustar os modelos de c√≥digo aberto para melhorar o desempenho em seu dom√≠nio espec√≠fico. Isso inclui a capacidade de treinar os modelos com seus dados espec√≠ficos, permitindo uma maior precis√£o e controle sobre os resultados. Com a combina√ß√£o de LLMs e ferramentas como a Databricks, voc√™ pode criar solu√ß√µes personalizadas para suas necessidades, desde a cria√ß√£o de modelos de linguagem at√© a implementa√ß√£o de solu√ß√µes de intelig√™ncia artificial em sua organiza√ß√£o.\n",
      "=== Calling Function ===\n",
      "Calling function: tutorial_engine with args: {\"input\": \"Quais as principais aplica\\u00e7\\u00f5es posso construir com LLMs e LangChain?\"}\n",
      "=== Function Output ===\n",
      "As principais aplica√ß√µes que podem ser constru√≠das com LLMs incluem:\n",
      "\n",
      "1. Cria√ß√£o e aprimoramento de conte√∫do, como:\n",
      " * Gera√ß√£o de conte√∫do: produ√ß√£o autom√°tica de texto\n",
      " * Assist√™ncia na reda√ß√£o: corre√ß√£o ortogr√°fica, de estilo e de conte√∫do\n",
      " * Tradu√ß√£o autom√°tica: convers√£o de texto de um idioma para outro\n",
      " * Resumo de textos: redu√ß√£o de documentos longos em resumos\n",
      " * Planejamento e roteiro de conte√∫do: estrutura√ß√£o do conte√∫do\n",
      " * Brainstorming: propostas criativas para projetos, nomes, conceitos, etc.\n",
      " * Programa√ß√£o: cria√ß√£o de c√≥digo de programa√ß√£o a partir de linguagem natural\n",
      "\n",
      "2. An√°lise e organiza√ß√£o de informa√ß√µes, como:\n",
      " * An√°lise de sentimento: avalia√ß√£o de emo√ß√µes e opini√µes em textos\n",
      " * Extra√ß√£o de informa√ß√µes: extra√ß√£o de dados espec√≠ficos de documentos grandes\n",
      " * Classifica√ß√£o de textos: organiza√ß√£o de textos em categorias ou temas espec√≠ficos\n",
      " * Revis√£o t√©cnica: assist√™ncia na revis√£o de documentos especializados\n",
      "\n",
      "3. Intera√ß√£o e automa√ß√£o, como:\n",
      " * Chatbots: simula√ß√£o de conversas sobre t√≥picos gerais ou espec√≠ficos\n",
      " * Perguntas e respostas: gera√ß√£o de respostas a perguntas com base em um corpus\n",
      "\n",
      "Al√©m disso, com o surgimento dos LLMs multimodais, outras aplica√ß√µes est√£o come√ßando a surgir, como a gera√ß√£o de conte√∫do audiovisual, a interpreta√ß√£o de dados de imagens, a tradu√ß√£o de conte√∫do multim√≠dia ou a cria√ß√£o de experi√™ncias interativas ricas.\n",
      "=== Calling Function ===\n",
      "Calling function: artigo_engine with args: {\"input\": \"Quais as principais aplica\\u00e7\\u00f5es posso construir com LLMs e LangChain?\"}\n",
      "=== Function Output ===\n",
      "Com LLMs, voc√™ pode desenvolver aplicativos prontos para produ√ß√£o, como modelos de linguagem personalizados para atender √†s necessidades espec√≠ficas do seu dom√≠nio. Al√©m disso, voc√™ pode ajustar os modelos de c√≥digo aberto para melhorar o desempenho em seu dom√≠nio espec√≠fico. Isso permite que voc√™ crie solu√ß√µes personalizadas para problemas como processamento de linguagem natural, gera√ß√£o de texto e an√°lise de dados. Com a combina√ß√£o de LLMs e ferramentas como a Databricks, voc√™ pode criar solu√ß√µes robustas e escal√°veis que atendam √†s suas necessidades de neg√≥cios.\n"
     ]
    }
   ],
   "source": [
    "response = agent_document.chat(\n",
    "    \"Quais as principais aplica√ß√µes posso construir com LLMs e LangChain?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Quais as principais tend√™ncias em LangChain e LLM?\n",
      "=== Calling Function ===\n",
      "Calling function: artigo_engine with args: {\"input\": \"Quais as principais tend\\u00eancias em LangChain e LLM?\"}\n",
      "=== Function Output ===\n",
      "N√£o h√° informa√ß√µes sobre tend√™ncias em LangChain e LLM. No entanto, para come√ßar a usar LLMs, existem algumas op√ß√µes dispon√≠veis, como assistir a apresenta√ß√µes sob demanda, participar de cursos sobre LLMs ou explorar exemplos de c√≥digo para desenvolver aplicativos prontos para produ√ß√£o com LLMs.\n",
      "=== Calling Function ===\n",
      "Calling function: tutorial_engine with args: {\"input\": \"Quais as principais tend\\u00eancias em LangChain e LLM?\"}\n",
      "=== Function Output ===\n",
      "As principais tend√™ncias em LangChain e LLM incluem a prolifera√ß√£o de LLMs de c√≥digo aberto, que democratizou o acesso √† tecnologia de ponta de processamento de linguagem, permitindo que pesquisadores, desenvolvedores e amadores experimentassem, personalizassem e implantassem solu√ß√µes de IA com um investimento inicial m√≠nimo. Al√©m disso, a integra√ß√£o do LLM √†s ferramentas de desenvolvimento de software e de escrit√≥rio est√° transformando a efici√™ncia e a capacidade das empresas. Outra tend√™ncia √© a evolu√ß√£o dos LLMs para al√©m da simples previs√£o de texto, tornando-se aplicativos sofisticados em v√°rios dom√≠nios, arquiteturas e modalidades, incluindo LLMs baseados em redes neurais recorrentes e transformers.\n",
      "=== Calling Function ===\n",
      "Calling function: artigo_engine with args: {\"input\": \"Quais as principais tend\\u00eancias em LangChain e LLM?\"}\n",
      "=== Function Output ===\n",
      "N√£o h√° informa√ß√µes sobre tend√™ncias em LangChain e LLM. No entanto, para come√ßar a usar LLMs, existem algumas op√ß√µes dispon√≠veis, como assistir a apresenta√ß√µes sob demanda, participar de cursos sobre LLMs ou explorar exemplos de c√≥digo para desenvolver aplicativos prontos para produ√ß√£o com LLMs.\n",
      "=== Calling Function ===\n",
      "Calling function: tutorial_engine with args: {\"input\": \"Quais as principais tend\\u00eancias em LangChain e LLM?\"}\n",
      "=== Function Output ===\n",
      "As principais tend√™ncias em LangChain e LLM incluem a prolifera√ß√£o de LLMs de c√≥digo aberto, que democratizou o acesso √† tecnologia de ponta de processamento de linguagem, permitindo que pesquisadores, desenvolvedores e amadores experimentassem, personalizassem e implantassem solu√ß√µes de IA com um investimento inicial m√≠nimo. Al√©m disso, a integra√ß√£o do LLM √†s ferramentas de desenvolvimento de software e de escrit√≥rio est√° transformando a efici√™ncia e a capacidade das empresas. Outra tend√™ncia √© a evolu√ß√£o dos LLMs para al√©m da simples previs√£o de texto, tornando-se aplicativos sofisticados em v√°rios dom√≠nios, arquiteturas e modalidades, incluindo LLMs baseados em redes neurais recorrentes (RNNs) e transformers.\n",
      "=== Calling Function ===\n",
      "Calling function: artigo_engine with args: {\"input\": \"Quais as principais tend\\u00eancias em LangChain e LLM?\"}\n",
      "=== Function Output ===\n",
      "N√£o h√° informa√ß√µes sobre tend√™ncias em LangChain e LLM no texto fornecido. O texto se concentra em fornecer op√ß√µes para come√ßar a usar LLMs, incluindo assistir a apresenta√ß√µes, fazer um curso e trabalhar com exemplos de c√≥digo. N√£o h√° men√ß√£o a tend√™ncias espec√≠ficas em LangChain e LLM.\n"
     ]
    }
   ],
   "source": [
    "response = agent_document.chat(\n",
    "    \"Quais as principais tend√™ncias em LangChain e LLM?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent ReAct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent import ReActAgent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = ReActAgent.from_tools(\n",
    "    query_engine_tools,\n",
    "    verbose=True,\n",
    "    llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Running step d4cc8851-0099-47c0-8dd2-1a880635fed8. Step input: Quais as principais ferramentas usadas em LAngChain?\n",
      "\u001b[1;3;38;5;200mThought: The current language of the user is: Portuguese. I need to use a tool to help me answer the question.\n",
      "Action: artigo_engine\n",
      "Action Input: {'input': 'Quais as principais ferramentas usadas em LangChain?'}\n",
      "\u001b[0m\u001b[1;3;34mObservation: N√£o h√° men√ß√£o √†s ferramentas usadas em LangChain no texto fornecido. O texto discute grandes modelos de linguagem (LLM), servi√ßos propriet√°rios como o ChatGPT e modelos de c√≥digo aberto, mas n√£o menciona LangChain ou suas ferramentas.\n",
      "\u001b[0m> Running step aa623f1b-2b32-4852-8e25-ab4b6e3fbf16. Step input: None\n",
      "\u001b[1;3;38;5;200mThought: A ferramenta artigo_engine n√£o forneceu informa√ß√µes suficientes sobre as principais ferramentas usadas em LangChain. Vou tentar novamente com a ferramenta tutorial_engine.\n",
      "Action: tutorial_engine\n",
      "Action Input: {'input': 'Quais as principais ferramentas usadas em LangChain?'}\n",
      "\u001b[0m\u001b[1;3;34mObservation: N√£o h√° men√ß√£o √†s principais ferramentas usadas em LangChain no texto fornecido. O texto discute sobre LLMs (Modelos de Linguagem Grande), sua evolu√ß√£o, aplica√ß√µes e tipologias, mas n√£o menciona LangChain ou suas ferramentas.\n",
      "\u001b[0m> Running step ce19d955-c6ea-4997-9bd8-d3554b582140. Step input: None\n",
      "\u001b[1;3;38;5;200mThought: As ferramentas artigo_engine e tutorial_engine n√£o forneceram informa√ß√µes suficientes sobre as principais ferramentas usadas em LangChain. Infelizmente, n√£o tenho mais informa√ß√µes para fornecer uma resposta precisa.\n",
      "Answer: Desculpe, mas n√£o foi poss√≠vel encontrar informa√ß√µes sobre as principais ferramentas usadas em LangChain com as ferramentas dispon√≠veis.\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "response = agent.chat(\n",
    "    \"Quais as principais ferramentas usadas em LAngChain?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Running step a0c4c1e6-6b3a-4252-b55a-866e30553cff. Step input: Quais as principais tend√™ncias em LAngChain que eu deveria estudar?\n",
      "\u001b[1;3;38;5;200mThought: O usu√°rio est√° procurando por tend√™ncias em LangChain. Eu posso usar o artigo_engine para obter informa√ß√µes sobre as principais tend√™ncias em LangChain.\n",
      "Action: artigo_engine\n",
      "Action Input: {'input': 'principais tend√™ncias em LangChain'}\n",
      "\u001b[0m\u001b[1;3;34mObservation: N√£o h√° men√ß√£o espec√≠fica √†s principais tend√™ncias em LangChain no texto fornecido. O texto discute os grandes modelos de linguagem (LLM), a import√¢ncia de ter controle sobre os dados e a capacidade de ajustar modelos de c√≥digo aberto para melhorar o desempenho em dom√≠nios espec√≠ficos, mas n√£o aborda o t√≥pico de LangChain.\n",
      "\u001b[0m> Running step dc43290c-1f27-4c96-b92c-032b5fcb1e51. Step input: None\n",
      "\u001b[1;3;38;5;200mThought: O artigo_engine n√£o forneceu informa√ß√µes sobre as principais tend√™ncias em LangChain. Vou tentar usar o tutorial_engine para obter informa√ß√µes sobre as principais tend√™ncias em LangChain.\n",
      "Action: tutorial_engine\n",
      "Action Input: {'input': 'principais tend√™ncias em LangChain'}\n",
      "\u001b[0m\u001b[1;3;34mObservation: N√£o h√° men√ß√£o espec√≠fica √†s principais tend√™ncias em LangChain no texto fornecido. No entanto, √© poss√≠vel identificar algumas tend√™ncias gerais relacionadas ao desenvolvimento e aplica√ß√£o de Large Language Models (LLMs) que podem estar relacionadas ao conceito de LangChain.\n",
      "\n",
      "Algumas das tend√™ncias observadas incluem:\n",
      "\n",
      "1. **Democratiza√ß√£o da tecnologia de IA**: A prolifera√ß√£o de LLMs de c√≥digo aberto est√° democratizando o acesso √† tecnologia de ponta de processamento de linguagem, permitindo que pesquisadores, desenvolvedores e amadores experimentem, personalizem e implantem solu√ß√µes de IA com um investimento inicial m√≠nimo.\n",
      "\n",
      "2. **Integra√ß√£o do LLM √†s ferramentas de desenvolvimento de software e de escrit√≥rio**: A integra√ß√£o do LLM √†s ferramentas de desenvolvimento de software e de escrit√≥rio est√° transformando a efici√™ncia e a capacidade das empresas, com exemplos como o Microsoft 365 Copilot e o Google Workspace.\n",
      "\n",
      "3. **Diversifica√ß√£o de arquiteturas de LLM**: Os LLMs est√£o progredindo al√©m da simples previs√£o de texto e se tornando aplicativos sofisticados em v√°rios dom√≠nios, arquiteturas e modalidades, incluindo LLMs baseados em redes neurais recorrentes (RNNs) e transformers.\n",
      "\n",
      "4. **Avan√ßo na regulamenta√ß√£o da IA**: H√° uma preocupa√ß√£o crescente com as considera√ß√µes e os desafios √©ticos apresentados pelo desenvolvimento e uso de LLMs, levando a um avan√ßo na regulamenta√ß√£o da IA e da IA generativa em todo o mundo.\n",
      "\n",
      "Essas tend√™ncias podem estar relacionadas ao conceito de LangChain, mas n√£o h√° informa√ß√µes espec√≠ficas sobre o tema no texto fornecido.\n",
      "\u001b[0m> Running step 00b0f793-f3b5-47cb-9e84-943ed051f1ee. Step input: None\n",
      "\u001b[1;3;38;5;200mThought: O tutorial_engine forneceu informa√ß√µes sobre tend√™ncias gerais relacionadas ao desenvolvimento e aplica√ß√£o de Large Language Models (LLMs) que podem estar relacionadas ao conceito de LangChain. Embora n√£o haja men√ß√£o espec√≠fica √†s principais tend√™ncias em LangChain, as tend√™ncias identificadas podem ser relevantes para o tema.\n",
      "Answer: Algumas das tend√™ncias gerais relacionadas ao desenvolvimento e aplica√ß√£o de Large Language Models (LLMs) que podem estar relacionadas ao conceito de LangChain incluem a democratiza√ß√£o da tecnologia de IA, a integra√ß√£o do LLM √†s ferramentas de desenvolvimento de software e de escrit√≥rio, a diversifica√ß√£o de arquiteturas de LLM e o avan√ßo na regulamenta√ß√£o da IA. Essas tend√™ncias podem ser relevantes para o tema de LangChain, mas n√£o h√° informa√ß√µes espec√≠ficas sobre o tema no texto fornecido.\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "response = agent.chat(\n",
    "    \"Quais as principais tend√™ncias em LAngChain que eu deveria estudar?\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
