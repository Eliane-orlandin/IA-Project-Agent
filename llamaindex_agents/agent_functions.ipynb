{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConfiguraÃ§Ã£o do Modelo de Linguagem (LLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/elianeorlandin/Documents/Desenvolvimento/IA-Project-Agent/llamaindex_agents/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Importa configuraÃ§Ãµes padrÃ£o do LlamaIndex\n",
    "from llama_index.core import Settings\n",
    "# Importa o LLM (modelo de linguagem) Groq, que serÃ¡ usado para processar as perguntas\n",
    "from llama_index.llms.groq import Groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importa biblioteca padrÃ£o para acessar variÃ¡veis de ambiente\n",
    "import os\n",
    "# Instancia o modelo de linguagem LLaMA 3.3 com 70 bilhÃµes de parÃ¢metros, usando a chave da API armazenada no sistema\n",
    "llm = Groq(model=\"llama-3.3-70b-versatile\", api_key=os.environ.get(\"GROQ_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FunÃ§Ã£o para CÃ¡lculo de Imposto de Renda\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FunÃ§Ã£o que calcula o imposto de renda com base no rendimento anual informado\n",
    "def calcular_imposto_renda(rendimento:float) -> str:\n",
    "    \"\"\" \n",
    "    Calcula o imposto de renda com base no rendimento anual.\n",
    "    Args:\n",
    "        rendimento (float): Rendimento anual do contribuinte.\n",
    "    Returns:\n",
    "        str: O valor do imposto devido com base no rendimento.\n",
    "    \"\"\"\n",
    "    if rendimento <= 2000:\n",
    "        return \"Isento de imposto de renda.\"\n",
    "    elif 2000 < rendimento <= 5000:\n",
    "        imposto = (rendimento - 2000) * 0.10\n",
    "        return f\"Imposto devido Ã© de: R$ {imposto:.2f}, base em um rendimento de R$ {rendimento:.2f}.\"\n",
    "    elif 5000 < rendimento <= 10000:\n",
    "        imposto = (rendimento - 5000) * 0.15 + 300\n",
    "        return f\"Imposto devido Ã© de: R$ {imposto:.2f}, base em um rendimento de R$ {rendimento:.2f}.\"\n",
    "    else:\n",
    "        imposto = (rendimento - 10000) * 0.20 + 1050\n",
    "        return f\"Imposto devido Ã© de: R$ {imposto:.2f}, base em um rendimento de R$ {rendimento:.2f}.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformando FunÃ§Ã£o em Ferramenta de Agente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforma a funÃ§Ã£o de cÃ¡lculo de imposto em uma ferramenta utilizÃ¡vel por um agente\n",
    "from llama_index.core.tools import FunctionTool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ferramenta_imposto_renda = FunctionTool.from_defaults(\n",
    "    fn=calcular_imposto_renda,\n",
    "    name=\"Calcular Imposto de Renda\",\n",
    "    description=(\n",
    "    \"Calcula o imposto de renda com base no rendimento anual do contribuinte.\"\n",
    "    \"Argumento: rendimento (float).\"\n",
    "    \"Retorna o valor do imposto devido de acordo com faixas de rendimento.\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CriaÃ§Ã£o e ExecuÃ§Ã£o de Agente com a Ferramenta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria um agente com capacidade de chamar funÃ§Ãµes (FunctionCallingAgentWorker)\n",
    "from llama_index.core.agent import FunctionCallingAgentWorker "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_worker_imposto = FunctionCallingAgentWorker.from_tools(\n",
    "    tools=[ferramenta_imposto_renda],\n",
    "    verbose=True,\n",
    "    allow_parallel_tool_calls=True,\n",
    "    llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Envolve o agente anterior com uma interface de conversaÃ§Ã£o\n",
    "from llama_index.core.agent import AgentRunner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_imposto= AgentRunner(agent_worker_imposto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: \n",
      "    Qual Ã© o imposto de renda devido por uma pessoa com rendimento anual de R$ 7500?\n",
      "\n",
      "=== Calling Function ===\n",
      "Calling function: Calcular Imposto de Renda with args: {\"rendimento\": 7500}\n",
      "=== Function Output ===\n",
      "Imposto devido Ã© de: R$ 675.00, base em um rendimento de R$ 7500.00.\n",
      "=== LLM Response ===\n",
      "O imposto de renda devido por uma pessoa com rendimento anual de R$ 7500 Ã© de R$ 675,00.\n"
     ]
    }
   ],
   "source": [
    "# Realiza a consulta ao agente: o agente vai identificar a funÃ§Ã£o adequada e chamÃ¡-la com os argumentos corretos\n",
    "response = agent_imposto.chat(\"\"\"\n",
    "    Qual Ã© o imposto de renda devido por uma pessoa com rendimento anual de R$ 7500?\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consulta de Artigos CientÃ­ficos no arXiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importa biblioteca para buscar artigos cientÃ­ficos no arXiv\n",
    "import arxiv \n",
    "\n",
    "# FunÃ§Ã£o que realiza a busca de artigos pelo tÃ­tulo no arXiv\n",
    "def consulta_artigos(titulo: str) -> str:\n",
    "    \"\"\" Consulta artigos na base de dados arXiv e retorna resultados formatados.\"\"\"\n",
    "    busca = arxiv.Search(\n",
    "        query=titulo,\n",
    "        max_results=5,\n",
    "        sort_by=arxiv.SortCriterion.Relevance\n",
    "    )\n",
    "    # Monta uma lista com os principais dados de cada artigo encontrado\n",
    "    resultados = [\n",
    "        f\"TÃ­tulo: {artigo.title}\\n\"\n",
    "        f\"Categoria: {artigo.primary_category}\\n\"\n",
    "        f\"Link: {artigo.entry_id}\\n\"\n",
    "        for artigo in busca.results()\n",
    "    ]\n",
    "\n",
    "    return \"\\n\\n\".join(resultados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforma a funÃ§Ã£o de busca de artigos em ferramenta compatÃ­vel com agente\n",
    "consulta_artigos_tool = FunctionTool.from_defaults(fn=consulta_artigos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria um novo agente que usa duas ferramentas: cÃ¡lculo de imposto e busca de artigos\n",
    "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
    "    [ferramenta_imposto_renda, consulta_artigos_tool],\n",
    "    verbose=True,\n",
    "    allow_parallel_tool_calls=False, # NÃ£o permite chamadas paralelas neste caso\n",
    "    llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Me retorne artigos sobre LangChain na eeducaÃ§Ã£o\n",
      "=== Calling Function ===\n",
      "Calling function: consulta_artigos with args: {\"titulo\": \"LangChain na educa\\u00e7\\u00e3o\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5z/bv195b9d0f11m9c66rw80j3r0000gp/T/ipykernel_10203/3605877752.py:15: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead\n",
      "  for artigo in busca.results()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Function Output ===\n",
      "TÃ­tulo: Development and Testing of Retrieval Augmented Generation in Large Language Models -- A Case Study Report\n",
      "Categoria: cs.CL\n",
      "Link: http://arxiv.org/abs/2402.01733v1\n",
      "\n",
      "\n",
      "TÃ­tulo: From Prompt Injections to SQL Injection Attacks: How Protected is Your LLM-Integrated Web Application?\n",
      "Categoria: cs.CR\n",
      "Link: http://arxiv.org/abs/2308.01990v4\n",
      "\n",
      "\n",
      "TÃ­tulo: Automating Customer Service using LangChain: Building custom open-source GPT Chatbot for organizations\n",
      "Categoria: cs.CL\n",
      "Link: http://arxiv.org/abs/2310.05421v1\n",
      "\n",
      "\n",
      "TÃ­tulo: Poisoned LangChain: Jailbreak LLMs by LangChain\n",
      "Categoria: cs.CL\n",
      "Link: http://arxiv.org/abs/2406.18122v1\n",
      "\n",
      "\n",
      "TÃ­tulo: Breast Ultrasound Report Generation using LangChain\n",
      "Categoria: eess.IV\n",
      "Link: http://arxiv.org/abs/2312.03013v1\n",
      "\n",
      "=== Calling Function ===\n",
      "Calling function: consulta_artigos with args: {\"titulo\": \"LangChain e educa\\u00e7\\u00e3o\"}\n",
      "=== Function Output ===\n",
      "TÃ­tulo: From Prompt Injections to SQL Injection Attacks: How Protected is Your LLM-Integrated Web Application?\n",
      "Categoria: cs.CR\n",
      "Link: http://arxiv.org/abs/2308.01990v4\n",
      "\n",
      "\n",
      "TÃ­tulo: Automating Customer Service using LangChain: Building custom open-source GPT Chatbot for organizations\n",
      "Categoria: cs.CL\n",
      "Link: http://arxiv.org/abs/2310.05421v1\n",
      "\n",
      "\n",
      "TÃ­tulo: Poisoned LangChain: Jailbreak LLMs by LangChain\n",
      "Categoria: cs.CL\n",
      "Link: http://arxiv.org/abs/2406.18122v1\n",
      "\n",
      "\n",
      "TÃ­tulo: Breast Ultrasound Report Generation using LangChain\n",
      "Categoria: eess.IV\n",
      "Link: http://arxiv.org/abs/2312.03013v1\n",
      "\n",
      "\n",
      "TÃ­tulo: Revolutionizing Mental Health Care through LangChain: A Journey with a Large Language Model\n",
      "Categoria: cs.HC\n",
      "Link: http://arxiv.org/abs/2403.05568v1\n",
      "\n",
      "=== Calling Function ===\n",
      "Calling function: consulta_artigos with args: {\"titulo\": \"LangChain aplicado \\u00e0 educa\\u00e7\\u00e3o\"}\n",
      "=== Function Output ===\n",
      "TÃ­tulo: From Prompt Injections to SQL Injection Attacks: How Protected is Your LLM-Integrated Web Application?\n",
      "Categoria: cs.CR\n",
      "Link: http://arxiv.org/abs/2308.01990v4\n",
      "\n",
      "\n",
      "TÃ­tulo: MÃ©todo de elementos finitos aplicado a las ecuaciones de Stokes y de AdvecciÃ³n-DifusiÃ³n\n",
      "Categoria: math.NA\n",
      "Link: http://arxiv.org/abs/1401.7619v1\n",
      "\n",
      "\n",
      "TÃ­tulo: MÃ©todo de Monte Carlo aplicado ao CÃ¡lculo FracionÃ¡rio\n",
      "Categoria: math.NA\n",
      "Link: http://arxiv.org/abs/2110.08129v1\n",
      "\n",
      "\n",
      "TÃ­tulo: Automating Customer Service using LangChain: Building custom open-source GPT Chatbot for organizations\n",
      "Categoria: cs.CL\n",
      "Link: http://arxiv.org/abs/2310.05421v1\n",
      "\n",
      "\n",
      "TÃ­tulo: Poisoned LangChain: Jailbreak LLMs by LangChain\n",
      "Categoria: cs.CL\n",
      "Link: http://arxiv.org/abs/2406.18122v1\n",
      "\n",
      "=== LLM Response ===\n",
      "Infelizmente, nÃ£o foi possÃ­vel encontrar artigos sobre LangChain aplicado Ã  educaÃ§Ã£o. No entanto, vocÃª pode tentar procurar em outras bases de dados ou realizar uma busca mais especÃ­fica para encontrar resultados mais relevantes. AlÃ©m disso, Ã© importante notar que a pesquisa sobre LangChain e sua aplicaÃ§Ã£o em diferentes Ã¡reas, incluindo a educaÃ§Ã£o, Ã© um campo em constante evoluÃ§Ã£o, e novos estudos e artigos podem ser publicados a qualquer momento.\n"
     ]
    }
   ],
   "source": [
    "# Cria um agente completo com as ferramentas acima\n",
    "agent = AgentRunner(agent_worker)\n",
    "# Exemplo de consulta ao agente: busca por artigos sobre LangChain na educaÃ§Ã£o\n",
    "response = agent.chat(\"Me retorne artigos sobre LangChain na educaÃ§Ã£o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IntegraÃ§Ã£o com Tavily para Busca CientÃ­fica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carrega variÃ¡veis de ambiente a partir de um arquivo `.env`\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ObtÃ©m chave da API Tavily\n",
    "tavily_key = os.environ.get(\"TAVILY_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importa ferramenta de busca da Tavily\n",
    "from llama_index.tools.tavily_research import TavilyToolSpec\n",
    "# Cria uma especificaÃ§Ã£o da ferramenta Tavily com a chave da API\n",
    "tavily_tool = TavilyToolSpec(\n",
    "    api_key=tavily_key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search\n"
     ]
    }
   ],
   "source": [
    "# Converte para uma lista de ferramentas compatÃ­veis com agentes\n",
    "tavily_tool_list = tavily_tool.to_tool_list()\n",
    "# Mostra o nome de cada ferramenta gerada pela Tavily\n",
    "for tool in tavily_tool_list:\n",
    "    print(tool.metadata.name)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id_='29cb1e12-0732-4fcc-ba58-101ad8f4f339', embedding=None, metadata={'url': 'https://community.revelo.com.br/faca-perguntas-ao-seu-pdf-usando-langchain-llama-2-e-python/'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Durante 2023, a Meta anunciou o LLaMA 2 (de agora em diante vou chamÃ¡-lo simplesmente de Llama 2), um LLM de cÃ³digo aberto que Ã© a evoluÃ§Ã£o de seu modelo anterior (LLaMA 1), que pode ser usado para criar aplicaÃ§Ãµes para fins comerciais. Para carregar o modelo Llama 2, como aconteceu com o carregamento de PDF e Pinecode, LangChain tambÃ©m nos fornece uma interface (que fÃ¡cil!). Isto nÃ£o significa que devolva dois parÃ¡grafos ou duas palavras, mas tem a ver com os textos que o modelo considera significativos para fornecer uma resposta: e um documento pode conter vÃ¡rios parÃ¡grafos. O que se vÃª neste artigo Ã© apenas um vislumbre das capacidades do LangChain, jÃ¡ que possui muitas outras integraÃ§Ãµes e, alÃ©m disso, permite que seja utilizado â€”atravÃ©s dos pluginsâ€” com outros modelos como ChatGPT.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Faz uma busca por artigos cientÃ­ficos sobre LangChain usando Tavily\n",
    "tavily_tool.search(\"Me rteorne artigos cientÃ­ficos sobre LangChain\", max_results=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DefiniÃ§Ã£o de ferramentas externas e integraÃ§Ã£o com o agente  \n",
    "Criamos uma ferramenta utilizando a API do Tavily para realizar buscas sobre um determinado tÃ³pico.  \n",
    "Essa ferramenta serÃ¡ incorporada ao agente para permitir pesquisas contextuais.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforma a funÃ§Ã£o Tavily em uma ferramenta diretamente utilizÃ¡vel\n",
    "from llama_index.core.tools import FunctionTool\n",
    "tavily_tool_function = FunctionTool.from_defaults(\n",
    "    fn=tavily_tool.search,\n",
    "    name=\"Tavily Search\",\n",
    "    description=(\n",
    "        \"Busca artigos com Tavily sobre um determinado tÃ³pico\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria agente com suporte Ã  ferramenta Tavily\n",
    "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
    "    tools=[tavily_tool_function],\n",
    "    verbose=True,\n",
    "    allow_parallel_tool_calls=False,\n",
    "    llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = AgentRunner(agent_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Me retorne artigos sobre LLM e LangChain\n",
      "=== Calling Function ===\n",
      "Calling function: Tavily Search with args: {\"max_results\": 6, \"query\": \"LLM e LangChain\"}\n",
      "=== Function Output ===\n",
      "[Document(id_='8c34e89a-924c-43fc-bd19-0d50e14e8d26', embedding=None, metadata={'url': 'https://www.techtarget.com/searchenterpriseai/tip/How-to-use-LangChain-for-LLM-application-development'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='[LangChain](https://www.techtarget.com/searchenterpriseai/definition/LangChain), an open source framework for building AI applications, has become a de facto standard for working with LLMs and integrating [APIs](https://www.techtarget.com/searchapparchitecture/definition/application-program-interface-API). The tool serves as a critical intermediary, enabling a targeted LLM to interface with traditional software. [...] LangChain enables developers to integrate AI models with standard IT components such as software utilities, APIs and databases. Within LangChain, developers use a combination of prompts, tools and chains to manage LLMs and create desired AI interactions.\\n\\n### Prompts\\n\\nAll processes in LangChain revolve around [prompts](https://www.techtarget.com/searchenterpriseai/definition/AI-prompt), which initiate the tasks that AI applications rely on. [...] Using LangChain with Spark and Kafka\\n------------------------------------\\n\\nLangChain excels at managing LLM workflows and integrating language models with APIs, tools and software utilities. But AI applications often require more than just model orchestration.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='0fc8abcf-f41b-4ba2-bc70-61fb6a41950a', embedding=None, metadata={'url': 'https://vitiya99.medium.com/building-scalable-llm-applications-with-langchain-and-vector-databases-5541c0ebee0e'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Large Language Models (LLMs) are transforming how we interact with information and build intelligent applications. However, effectively leveraging their power for real-world scenarios often requires extending their capabilities beyond simple prompt-response interactions. This is where LangChain and vector databases come into play. LangChain provides a streamlined framework for developing LLM-powered applications, while vector databases facilitate efficient semantic search, enabling LLMs to [...] LangChain simplifies the complexities of integrating LLMs into applications by providing abstractions for common tasks like prompt management, chain execution, and memory management. Vector databases, such as Pinecone, Weaviate, and Faiss, store embeddings (vector representations of data) generated by models like Sentence Transformers or OpenAIâ€™s embeddings API. This allows for similarity search, enabling the retrieval of contextually relevant information based on semantic meaning rather than [...] keyword matching. The combination of LangChain and vector databases empowers developers to build sophisticated LLM applications that can access, process, and reason over large amounts of unstructured data.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='c00ed611-1b03-4cf4-8dc8-77a9dd241cee', embedding=None, metadata={'url': 'https://scalexi.medium.com/understanding-the-differences-between-llm-chains-and-llm-agent-executors-in-langchain-3f3cf402442f'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=\"**LangChain** has emerged as a robust framework for building applications powered by large language models (LLMs). Two fundamental concepts within LangChain are **LLM Chains** and **LLM Agent Executors**, both of which leverage tools to enhance the capabilities of LLMs. While they may seem similar at first glance, understanding their differences is crucial for developers aiming to harness LangChain's full potential. [...] Both **LLM Chains** and **LLM Agent Executors** offer powerful ways to structure and execute tasks using LangChain, but they are designed for different use cases. Understanding the key differences is essential for choosing the right approach.\", path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='88498561-b8f8-4e79-b22d-aca3e7dd2392', embedding=None, metadata={'url': 'https://python.langchain.com/docs/integrations/llms/'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Unless you are specifically using more advanced prompting techniques, you are probably looking for [this page instead](https://python.langchain.com/docs/integrations/chat/).\\n\\n[LLMs](https://python.langchain.com/docs/concepts/text_llms/) are language models that take a string as input and return a string as output.\\n\\ninfo [...] [![Image 2: Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/integrations/llms/index.mdx)\\n\\nLLMs\\n====\\n\\ncaution\\n\\nYou are currently on a page documenting the use of [text completion models](https://python.langchain.com/docs/concepts/text_llms/). Many of the latest and most popular models are [chat completion models](https://python.langchain.com/docs/concepts/chat_models/). [...] *   [Kinetica](https://python.langchain.com/docs/integrations/chat/kinetica/)\\n        *   [Konko](https://python.langchain.com/docs/integrations/chat/konko/)\\n        *   [LiteLLM](https://python.langchain.com/docs/integrations/chat/litellm/)\\n        *   [Llama 2 Chat](https://python.langchain.com/docs/integrations/chat/llama2_chat/)\\n        *   [Llama API](https://python.langchain.com/docs/integrations/chat/llama_api/)', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='999aa9f9-ca32-4830-bc52-7044d316cbce', embedding=None, metadata={'url': 'https://python.langchain.com/docs/tutorials/llm_chain/'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=\"In this quickstart we'll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it's just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!\\n\\nAfter reading this tutorial, you'll have a high level overview of: [...] Many of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with [LangSmith](https://smith.langchain.com/).\\n\\nAfter you sign up at the link above, make sure to set your environment variables to start logging traces: [...] [![Image 1: ðŸ¦œï¸ðŸ”— LangChain](https://python.langchain.com/img/brand/wordmark.png)](https://python.langchain.com/)[Integrations](https://python.langchain.com/docs/integrations/providers/)[API Reference](https://python.langchain.com/api_reference/)\\n\\n[More](https://python.langchain.com/docs/tutorials/llm_chain/#)\", path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='d90d9fb5-b2d6-4d4e-96ec-7e303ab941fa', embedding=None, metadata={'url': 'https://vstorm.co/the-power-of-langchain-in-llm-based-applications/'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='LangChain, a framework specifically designed for Large Language Model (LLM) applications, has emerged as a major tool in enhancing the capabilities of natural', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}')]\n",
      "=== LLM Response ===\n",
      "Os artigos sobre LLM e LangChain abordam a integraÃ§Ã£o de modelos de linguagem grandes (LLMs) com a framework LangChain para desenvolver aplicaÃ§Ãµes de inteligÃªncia artificial (IA) mais avanÃ§adas. A LangChain fornece uma estrutura para gerenciar workflows de LLMs e integrÃ¡-los com APIs, ferramentas e utilitÃ¡rios de software, permitindo que os desenvolvedores criem aplicaÃ§Ãµes de IA mais complexas e escalÃ¡veis.\n",
      "\n",
      "Os artigos destacam a importÃ¢ncia da LangChain para simplificar a integraÃ§Ã£o de LLMs em aplicaÃ§Ãµes, fornecendo abstraÃ§Ãµes para tarefas comuns como gerenciamento de prompts, execuÃ§Ã£o de chains e gerenciamento de memÃ³ria. AlÃ©m disso, a combinaÃ§Ã£o da LangChain com bancos de dados de vetores permite a realizaÃ§Ã£o de buscas semÃ¢nticas eficientes, permitindo que os LLMs acessem e processem grandes quantidades de dados nÃ£o estruturados.\n",
      "\n",
      "Os artigos tambÃ©m discutem a diferenÃ§a entre LLM Chains e LLM Agent Executors, dois conceitos fundamentais na LangChain, e como eles podem ser usados para estruturar e executar tarefas em aplicaÃ§Ãµes de IA. AlÃ©m disso, os artigos fornecem exemplos de como a LangChain pode ser usada para desenvolver aplicaÃ§Ãµes de IA, como traduÃ§Ã£o de texto e busca semÃ¢ntica.\n",
      "\n",
      "Em resumo, os artigos sobre LLM e LangChain destacam a importÃ¢ncia da integraÃ§Ã£o de modelos de linguagem grandes com a framework LangChain para desenvolver aplicaÃ§Ãµes de inteligÃªncia artificial mais avanÃ§adas e escalÃ¡veis. A LangChain fornece uma estrutura para gerenciar workflows de LLMs e integrÃ¡-los com APIs, ferramentas e utilitÃ¡rios de software, permitindo que os desenvolvedores criem aplicaÃ§Ãµes de IA mais complexas e eficientes.\n"
     ]
    }
   ],
   "source": [
    "# Consulta feita ao agente usando Tavily\n",
    "response = agent.chat(\"Me retorne artigos sobre LLM e LangChain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Os artigos sobre LLM e LangChain abordam a integraÃ§Ã£o de modelos de linguagem grandes (LLMs) com a framework LangChain para desenvolver aplicaÃ§Ãµes de inteligÃªncia artificial (IA) mais avanÃ§adas. A LangChain fornece uma estrutura para gerenciar workflows de LLMs e integrÃ¡-los com APIs, ferramentas e utilitÃ¡rios de software, permitindo que os desenvolvedores criem aplicaÃ§Ãµes de IA mais complexas e escalÃ¡veis.\n",
      "\n",
      "Os artigos destacam a importÃ¢ncia da LangChain para simplificar a integraÃ§Ã£o de LLMs em aplicaÃ§Ãµes, fornecendo abstraÃ§Ãµes para tarefas comuns como gerenciamento de prompts, execuÃ§Ã£o de chains e gerenciamento de memÃ³ria. AlÃ©m disso, a combinaÃ§Ã£o da LangChain com bancos de dados de vetores permite a realizaÃ§Ã£o de buscas semÃ¢nticas eficientes, permitindo que os LLMs acessem e processem grandes quantidades de dados nÃ£o estruturados.\n",
      "\n",
      "Os artigos tambÃ©m discutem a diferenÃ§a entre LLM Chains e LLM Agent Executors, dois conceitos fundamentais na LangChain, e como eles podem ser usados para estruturar e executar tarefas em aplicaÃ§Ãµes de IA. AlÃ©m disso, os artigos fornecem exemplos de como a LangChain pode ser usada para desenvolver aplicaÃ§Ãµes de IA, como traduÃ§Ã£o de texto e busca semÃ¢ntica.\n",
      "\n",
      "Em resumo, os artigos sobre LLM e LangChain destacam a importÃ¢ncia da integraÃ§Ã£o de modelos de linguagem grandes com a framework LangChain para desenvolver aplicaÃ§Ãµes de inteligÃªncia artificial mais avanÃ§adas e escalÃ¡veis. A LangChain fornece uma estrutura para gerenciar workflows de LLMs e integrÃ¡-los com APIs, ferramentas e utilitÃ¡rios de software, permitindo que os desenvolvedores criem aplicaÃ§Ãµes de IA mais complexas e eficientes.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IndexaÃ§Ã£o e Consulta de PDFs Locais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importa leitor de arquivos para leitura de PDFs locais\n",
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 16 0 (offset 0)\n",
      "Ignoring wrong pointing object 18 0 (offset 0)\n",
      "Ignoring wrong pointing object 20 0 (offset 0)\n",
      "Ignoring wrong pointing object 22 0 (offset 0)\n",
      "Ignoring wrong pointing object 42 0 (offset 0)\n",
      "Ignoring wrong pointing object 50 0 (offset 0)\n",
      "Ignoring wrong pointing object 52 0 (offset 0)\n",
      "Ignoring wrong pointing object 54 0 (offset 0)\n",
      "Ignoring wrong pointing object 56 0 (offset 0)\n",
      "Ignoring wrong pointing object 58 0 (offset 0)\n",
      "Ignoring wrong pointing object 70 0 (offset 0)\n",
      "Ignoring wrong pointing object 72 0 (offset 0)\n",
      "Ignoring wrong pointing object 89 0 (offset 0)\n",
      "Ignoring wrong pointing object 91 0 (offset 0)\n",
      "Ignoring wrong pointing object 103 0 (offset 0)\n",
      "Ignoring wrong pointing object 108 0 (offset 0)\n",
      "Ignoring wrong pointing object 149 0 (offset 0)\n",
      "Ignoring wrong pointing object 155 0 (offset 0)\n",
      "Ignoring wrong pointing object 158 0 (offset 0)\n",
      "Ignoring wrong pointing object 160 0 (offset 0)\n",
      "Ignoring wrong pointing object 163 0 (offset 0)\n",
      "Ignoring wrong pointing object 165 0 (offset 0)\n"
     ]
    }
   ],
   "source": [
    "# LÃª dois arquivos PDF com conteÃºdos sobre LLM\n",
    "url = \"files/LLM.pdf\"\n",
    "artigo = SimpleDirectoryReader(input_files=[url]).load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"files/LLM_2.pdf\"\n",
    "tutorial = SimpleDirectoryReader(input_files=[url]).load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gerar os Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configura modelo de embeddings para indexaÃ§Ã£o dos textos\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "Settings.embed_model = HuggingFaceEmbedding(\n",
    "    model_name= \"inTfloat/multilingual-e5-large\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria Ã­ndice vetorial para pesquisa semÃ¢ntica dos documentos\n",
    "\n",
    "artigo_index = VectorStoreIndex.from_documents(artigo)\n",
    "tutorial_index = VectorStoreIndex.from_documents(tutorial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Persiste os Ã­ndices localmente para reutilizaÃ§Ã£o futura\n",
    "artigo_index.storage_context.persist(persist_dir=\"artigo\")\n",
    "tutorial_index.storage_context.persist(persist_dir=\"tutorial\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Engine de Busca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carrega os Ã­ndices previamente salvos do disco\n",
    "from llama_index.core import StorageContext, load_index_from_storage    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_context = StorageContext.from_defaults(\n",
    "    persist_dir=\"artigo\"\n",
    ")\n",
    "artigo_index = load_index_from_storage(storage_context)\n",
    "\n",
    "storage_context = StorageContext.from_defaults(\n",
    "    persist_dir=\"tutorial\"\n",
    ")\n",
    "tutorial_index = load_index_from_storage(storage_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria motores de consulta com suporte ao modelo de linguagem para os dois Ã­ndices\n",
    "artigo_engine = artigo_index.as_query_engine(similary_top_k=3, llm=llm)\n",
    "tutorial_engine = tutorial_index.as_query_engine(similary_top_k=3, llm=llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CriaÃ§Ã£o de Agentes com Busca em Documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria ferramentas de consulta semÃ¢ntica para os documentos\n",
    "from llama_index.core.tools import QueryEngineTool, ToolMetadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine_tools = [\n",
    "    QueryEngineTool(\n",
    "        query_engine=artigo_engine,\n",
    "        metadata=ToolMetadata(\n",
    "            name=\"artigo_engine\",\n",
    "            description=(\n",
    "                \"Fornece informaÃ§Ãµe4s sobre LLM e LangChain.\"\n",
    "                \"Use uma pergunta detalhada em texto simples como entrada para a ferramenta\"\n",
    "            )\n",
    "        )\n",
    "    ),\n",
    "    QueryEngineTool(\n",
    "        query_engine=tutorial_engine,\n",
    "        metadata=ToolMetadata(\n",
    "            name=\"tutorial_engine\",\n",
    "            description=(\n",
    "                \"Fornece informaÃ§Ãµe4s sobre casos de uso e aplicaÃ§Ãµes em LLMs.\"\n",
    "                \"Use uma pergunta detalhada em texto simples como entrada para a ferramenta\"\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria agente que pode consultar os dois motores de busca semÃ¢ntica\n",
    "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
    "    query_engine_tools,\n",
    "    verbose=True,\n",
    "    allow_parallel_tool_calls=True,\n",
    "    llm=llm\n",
    ")\n",
    "agent_document = AgentRunner(agent_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Quais as principais aplicaÃ§Ãµes posso construir com LLMs e LangChain?\n",
      "=== Calling Function ===\n",
      "Calling function: artigo_engine with args: {\"input\": \"Quais as principais aplica\\u00e7\\u00f5es posso construir com LLMs e LangChain?\"}\n",
      "=== Function Output ===\n",
      "Com LLMs, vocÃª pode desenvolver aplicativos prontos para produÃ§Ã£o, como modelos de linguagem personalizados para atender Ã s necessidades especÃ­ficas do seu domÃ­nio. AlÃ©m disso, vocÃª pode ajustar os modelos de cÃ³digo aberto para melhorar o desempenho em seu domÃ­nio especÃ­fico. Isso inclui a capacidade de treinar os modelos com seus dados especÃ­ficos, permitindo uma maior precisÃ£o e controle sobre os resultados. Com a combinaÃ§Ã£o de LLMs e ferramentas como a Databricks, vocÃª pode criar soluÃ§Ãµes personalizadas para suas necessidades, desde a criaÃ§Ã£o de modelos de linguagem atÃ© a implementaÃ§Ã£o de soluÃ§Ãµes de dados.\n",
      "=== Calling Function ===\n",
      "Calling function: tutorial_engine with args: {\"input\": \"Quais as principais aplica\\u00e7\\u00f5es posso construir com LLMs e LangChain?\"}\n",
      "=== Function Output ===\n",
      "As principais aplicaÃ§Ãµes que podem ser construÃ­das com LLMs incluem:\n",
      "\n",
      "1. CriaÃ§Ã£o e aprimoramento de conteÃºdo, como:\n",
      " * GeraÃ§Ã£o de conteÃºdo: produÃ§Ã£o automÃ¡tica de texto\n",
      " * AssistÃªncia na redaÃ§Ã£o: correÃ§Ã£o ortogrÃ¡fica, de estilo e de conteÃºdo\n",
      " * TraduÃ§Ã£o automÃ¡tica: conversÃ£o de texto de um idioma para outro\n",
      " * Resumo de textos: reduÃ§Ã£o de documentos longos em resumos\n",
      " * Planejamento e roteiro de conteÃºdo: estruturaÃ§Ã£o do conteÃºdo\n",
      " * Brainstorming: propostas criativas para projetos, nomes, conceitos, etc.\n",
      " * ProgramaÃ§Ã£o: criaÃ§Ã£o de cÃ³digo de programaÃ§Ã£o a partir de linguagem natural\n",
      "\n",
      "2. AnÃ¡lise e organizaÃ§Ã£o de informaÃ§Ãµes, como:\n",
      " * AnÃ¡lise de sentimento: avaliaÃ§Ã£o de emoÃ§Ãµes e opiniÃµes em textos\n",
      " * ExtraÃ§Ã£o de informaÃ§Ãµes: extraÃ§Ã£o de dados especÃ­ficos de documentos grandes\n",
      " * ClassificaÃ§Ã£o de textos: organizaÃ§Ã£o de textos em categorias ou temas especÃ­ficos\n",
      " * RevisÃ£o tÃ©cnica: assistÃªncia na revisÃ£o de documentos especializados\n",
      "\n",
      "3. InteraÃ§Ã£o e automaÃ§Ã£o, como:\n",
      " * Chatbots: simulaÃ§Ã£o de conversas sobre tÃ³picos gerais ou especÃ­ficos\n",
      " * Perguntas e respostas: geraÃ§Ã£o de respostas a perguntas com base em um corpus\n",
      "\n",
      "AlÃ©m disso, com o surgimento dos LLMs multimodais, outras aplicaÃ§Ãµes estÃ£o comeÃ§ando a surgir, como a geraÃ§Ã£o de conteÃºdo audiovisual, a interpretaÃ§Ã£o de dados de imagens, a traduÃ§Ã£o de conteÃºdo multimÃ­dia ou a criaÃ§Ã£o de experiÃªncias interativas ricas.\n",
      "=== Calling Function ===\n",
      "Calling function: artigo_engine with args: {\"input\": \"Quais as principais aplica\\u00e7\\u00f5es posso construir com LLMs e LangChain?\"}\n",
      "=== Function Output ===\n",
      "Com LLMs, vocÃª pode desenvolver aplicativos prontos para produÃ§Ã£o, como modelos de linguagem personalizados para atender Ã s necessidades especÃ­ficas do seu domÃ­nio. AlÃ©m disso, vocÃª pode ajustar os modelos de cÃ³digo aberto para melhorar o desempenho em seu domÃ­nio especÃ­fico. Isso inclui a capacidade de treinar os modelos com seus dados especÃ­ficos, permitindo uma maior precisÃ£o e controle sobre os resultados. Com a combinaÃ§Ã£o de LLMs e ferramentas como a Databricks, vocÃª pode criar soluÃ§Ãµes personalizadas para suas necessidades, desde a criaÃ§Ã£o de modelos de linguagem atÃ© a implementaÃ§Ã£o de soluÃ§Ãµes de inteligÃªncia artificial em sua organizaÃ§Ã£o.\n",
      "=== Calling Function ===\n",
      "Calling function: tutorial_engine with args: {\"input\": \"Quais as principais aplica\\u00e7\\u00f5es posso construir com LLMs e LangChain?\"}\n",
      "=== Function Output ===\n",
      "As principais aplicaÃ§Ãµes que podem ser construÃ­das com LLMs incluem:\n",
      "\n",
      "1. CriaÃ§Ã£o e aprimoramento de conteÃºdo, como:\n",
      " * GeraÃ§Ã£o de conteÃºdo: produÃ§Ã£o automÃ¡tica de texto\n",
      " * AssistÃªncia na redaÃ§Ã£o: correÃ§Ã£o ortogrÃ¡fica, de estilo e de conteÃºdo\n",
      " * TraduÃ§Ã£o automÃ¡tica: conversÃ£o de texto de um idioma para outro\n",
      " * Resumo de textos: reduÃ§Ã£o de documentos longos em resumos\n",
      " * Planejamento e roteiro de conteÃºdo: estruturaÃ§Ã£o do conteÃºdo\n",
      " * Brainstorming: propostas criativas para projetos, nomes, conceitos, etc.\n",
      " * ProgramaÃ§Ã£o: criaÃ§Ã£o de cÃ³digo de programaÃ§Ã£o a partir de linguagem natural\n",
      "\n",
      "2. AnÃ¡lise e organizaÃ§Ã£o de informaÃ§Ãµes, como:\n",
      " * AnÃ¡lise de sentimento: avaliaÃ§Ã£o de emoÃ§Ãµes e opiniÃµes em textos\n",
      " * ExtraÃ§Ã£o de informaÃ§Ãµes: extraÃ§Ã£o de dados especÃ­ficos de documentos grandes\n",
      " * ClassificaÃ§Ã£o de textos: organizaÃ§Ã£o de textos em categorias ou temas especÃ­ficos\n",
      " * RevisÃ£o tÃ©cnica: assistÃªncia na revisÃ£o de documentos especializados\n",
      "\n",
      "3. InteraÃ§Ã£o e automaÃ§Ã£o, como:\n",
      " * Chatbots: simulaÃ§Ã£o de conversas sobre tÃ³picos gerais ou especÃ­ficos\n",
      " * Perguntas e respostas: geraÃ§Ã£o de respostas a perguntas com base em um corpus\n",
      "\n",
      "AlÃ©m disso, com o surgimento dos LLMs multimodais, outras aplicaÃ§Ãµes estÃ£o comeÃ§ando a surgir, como a geraÃ§Ã£o de conteÃºdo audiovisual, a interpretaÃ§Ã£o de dados de imagens, a traduÃ§Ã£o de conteÃºdo multimÃ­dia ou a criaÃ§Ã£o de experiÃªncias interativas ricas.\n",
      "=== Calling Function ===\n",
      "Calling function: artigo_engine with args: {\"input\": \"Quais as principais aplica\\u00e7\\u00f5es posso construir com LLMs e LangChain?\"}\n",
      "=== Function Output ===\n",
      "Com LLMs, vocÃª pode desenvolver aplicativos prontos para produÃ§Ã£o, como modelos de linguagem personalizados para atender Ã s necessidades especÃ­ficas do seu domÃ­nio. AlÃ©m disso, vocÃª pode ajustar os modelos de cÃ³digo aberto para melhorar o desempenho em seu domÃ­nio especÃ­fico. Isso permite que vocÃª crie soluÃ§Ãµes personalizadas para problemas como processamento de linguagem natural, geraÃ§Ã£o de texto e anÃ¡lise de dados. Com a combinaÃ§Ã£o de LLMs e ferramentas como a Databricks, vocÃª pode criar soluÃ§Ãµes robustas e escalÃ¡veis que atendam Ã s suas necessidades de negÃ³cios.\n"
     ]
    }
   ],
   "source": [
    "# Consulta ao agente: pergunta sobre aplicaÃ§Ãµes de LLMs e LangChain\n",
    "response = agent_document.chat(\n",
    "    \"Quais as principais aplicaÃ§Ãµes posso construir com LLMs e LangChain?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Quais as principais tendÃªncias em LangChain e LLM?\n",
      "=== Calling Function ===\n",
      "Calling function: artigo_engine with args: {\"input\": \"Quais as principais tend\\u00eancias em LangChain e LLM?\"}\n",
      "=== Function Output ===\n",
      "NÃ£o hÃ¡ informaÃ§Ãµes sobre tendÃªncias em LangChain e LLM. No entanto, para comeÃ§ar a usar LLMs, existem algumas opÃ§Ãµes disponÃ­veis, como assistir a apresentaÃ§Ãµes sob demanda, participar de cursos sobre LLMs ou explorar exemplos de cÃ³digo para desenvolver aplicativos prontos para produÃ§Ã£o com LLMs.\n",
      "=== Calling Function ===\n",
      "Calling function: tutorial_engine with args: {\"input\": \"Quais as principais tend\\u00eancias em LangChain e LLM?\"}\n",
      "=== Function Output ===\n",
      "As principais tendÃªncias em LangChain e LLM incluem a proliferaÃ§Ã£o de LLMs de cÃ³digo aberto, que democratizou o acesso Ã  tecnologia de ponta de processamento de linguagem, permitindo que pesquisadores, desenvolvedores e amadores experimentassem, personalizassem e implantassem soluÃ§Ãµes de IA com um investimento inicial mÃ­nimo. AlÃ©m disso, a integraÃ§Ã£o do LLM Ã s ferramentas de desenvolvimento de software e de escritÃ³rio estÃ¡ transformando a eficiÃªncia e a capacidade das empresas. Outra tendÃªncia Ã© a evoluÃ§Ã£o dos LLMs para alÃ©m da simples previsÃ£o de texto, tornando-se aplicativos sofisticados em vÃ¡rios domÃ­nios, arquiteturas e modalidades, incluindo LLMs baseados em redes neurais recorrentes e transformers.\n",
      "=== Calling Function ===\n",
      "Calling function: artigo_engine with args: {\"input\": \"Quais as principais tend\\u00eancias em LangChain e LLM?\"}\n",
      "=== Function Output ===\n",
      "NÃ£o hÃ¡ informaÃ§Ãµes sobre tendÃªncias em LangChain e LLM. No entanto, para comeÃ§ar a usar LLMs, existem algumas opÃ§Ãµes disponÃ­veis, como assistir a apresentaÃ§Ãµes sob demanda, participar de cursos sobre LLMs ou explorar exemplos de cÃ³digo para desenvolver aplicativos prontos para produÃ§Ã£o com LLMs.\n",
      "=== Calling Function ===\n",
      "Calling function: tutorial_engine with args: {\"input\": \"Quais as principais tend\\u00eancias em LangChain e LLM?\"}\n",
      "=== Function Output ===\n",
      "As principais tendÃªncias em LangChain e LLM incluem a proliferaÃ§Ã£o de LLMs de cÃ³digo aberto, que democratizou o acesso Ã  tecnologia de ponta de processamento de linguagem, permitindo que pesquisadores, desenvolvedores e amadores experimentassem, personalizassem e implantassem soluÃ§Ãµes de IA com um investimento inicial mÃ­nimo. AlÃ©m disso, a integraÃ§Ã£o do LLM Ã s ferramentas de desenvolvimento de software e de escritÃ³rio estÃ¡ transformando a eficiÃªncia e a capacidade das empresas. Outra tendÃªncia Ã© a evoluÃ§Ã£o dos LLMs para alÃ©m da simples previsÃ£o de texto, tornando-se aplicativos sofisticados em vÃ¡rios domÃ­nios, arquiteturas e modalidades, incluindo LLMs baseados em redes neurais recorrentes (RNNs) e transformers.\n",
      "=== Calling Function ===\n",
      "Calling function: artigo_engine with args: {\"input\": \"Quais as principais tend\\u00eancias em LangChain e LLM?\"}\n",
      "=== Function Output ===\n",
      "NÃ£o hÃ¡ informaÃ§Ãµes sobre tendÃªncias em LangChain e LLM no texto fornecido. O texto se concentra em fornecer opÃ§Ãµes para comeÃ§ar a usar LLMs, incluindo assistir a apresentaÃ§Ãµes, fazer um curso e trabalhar com exemplos de cÃ³digo. NÃ£o hÃ¡ menÃ§Ã£o a tendÃªncias especÃ­ficas em LangChain e LLM.\n"
     ]
    }
   ],
   "source": [
    "response = agent_document.chat(\n",
    "    \"Quais as principais tendÃªncias em LangChain e LLM?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uso de Agente do Tipo ReAct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UtilizaÃ§Ã£o de outro tipo de agente: ReActAgent, que combina raciocÃ­nio com aÃ§Ã£o\n",
    "from llama_index.core.agent import ReActAgent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = ReActAgent.from_tools(\n",
    "    query_engine_tools,\n",
    "    verbose=True,\n",
    "    llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Running step d4cc8851-0099-47c0-8dd2-1a880635fed8. Step input: Quais as principais ferramentas usadas em LAngChain?\n",
      "\u001b[1;3;38;5;200mThought: The current language of the user is: Portuguese. I need to use a tool to help me answer the question.\n",
      "Action: artigo_engine\n",
      "Action Input: {'input': 'Quais as principais ferramentas usadas em LangChain?'}\n",
      "\u001b[0m\u001b[1;3;34mObservation: NÃ£o hÃ¡ menÃ§Ã£o Ã s ferramentas usadas em LangChain no texto fornecido. O texto discute grandes modelos de linguagem (LLM), serviÃ§os proprietÃ¡rios como o ChatGPT e modelos de cÃ³digo aberto, mas nÃ£o menciona LangChain ou suas ferramentas.\n",
      "\u001b[0m> Running step aa623f1b-2b32-4852-8e25-ab4b6e3fbf16. Step input: None\n",
      "\u001b[1;3;38;5;200mThought: A ferramenta artigo_engine nÃ£o forneceu informaÃ§Ãµes suficientes sobre as principais ferramentas usadas em LangChain. Vou tentar novamente com a ferramenta tutorial_engine.\n",
      "Action: tutorial_engine\n",
      "Action Input: {'input': 'Quais as principais ferramentas usadas em LangChain?'}\n",
      "\u001b[0m\u001b[1;3;34mObservation: NÃ£o hÃ¡ menÃ§Ã£o Ã s principais ferramentas usadas em LangChain no texto fornecido. O texto discute sobre LLMs (Modelos de Linguagem Grande), sua evoluÃ§Ã£o, aplicaÃ§Ãµes e tipologias, mas nÃ£o menciona LangChain ou suas ferramentas.\n",
      "\u001b[0m> Running step ce19d955-c6ea-4997-9bd8-d3554b582140. Step input: None\n",
      "\u001b[1;3;38;5;200mThought: As ferramentas artigo_engine e tutorial_engine nÃ£o forneceram informaÃ§Ãµes suficientes sobre as principais ferramentas usadas em LangChain. Infelizmente, nÃ£o tenho mais informaÃ§Ãµes para fornecer uma resposta precisa.\n",
      "Answer: Desculpe, mas nÃ£o foi possÃ­vel encontrar informaÃ§Ãµes sobre as principais ferramentas usadas em LangChain com as ferramentas disponÃ­veis.\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Pergunta feita ao agente com raciocÃ­nio + ferramenta\n",
    "response = agent.chat(\n",
    "    \"Quais as principais ferramentas usadas em LAngChain?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Running step a0c4c1e6-6b3a-4252-b55a-866e30553cff. Step input: Quais as principais tendÃªncias em LAngChain que eu deveria estudar?\n",
      "\u001b[1;3;38;5;200mThought: O usuÃ¡rio estÃ¡ procurando por tendÃªncias em LangChain. Eu posso usar o artigo_engine para obter informaÃ§Ãµes sobre as principais tendÃªncias em LangChain.\n",
      "Action: artigo_engine\n",
      "Action Input: {'input': 'principais tendÃªncias em LangChain'}\n",
      "\u001b[0m\u001b[1;3;34mObservation: NÃ£o hÃ¡ menÃ§Ã£o especÃ­fica Ã s principais tendÃªncias em LangChain no texto fornecido. O texto discute os grandes modelos de linguagem (LLM), a importÃ¢ncia de ter controle sobre os dados e a capacidade de ajustar modelos de cÃ³digo aberto para melhorar o desempenho em domÃ­nios especÃ­ficos, mas nÃ£o aborda o tÃ³pico de LangChain.\n",
      "\u001b[0m> Running step dc43290c-1f27-4c96-b92c-032b5fcb1e51. Step input: None\n",
      "\u001b[1;3;38;5;200mThought: O artigo_engine nÃ£o forneceu informaÃ§Ãµes sobre as principais tendÃªncias em LangChain. Vou tentar usar o tutorial_engine para obter informaÃ§Ãµes sobre as principais tendÃªncias em LangChain.\n",
      "Action: tutorial_engine\n",
      "Action Input: {'input': 'principais tendÃªncias em LangChain'}\n",
      "\u001b[0m\u001b[1;3;34mObservation: NÃ£o hÃ¡ menÃ§Ã£o especÃ­fica Ã s principais tendÃªncias em LangChain no texto fornecido. No entanto, Ã© possÃ­vel identificar algumas tendÃªncias gerais relacionadas ao desenvolvimento e aplicaÃ§Ã£o de Large Language Models (LLMs) que podem estar relacionadas ao conceito de LangChain.\n",
      "\n",
      "Algumas das tendÃªncias observadas incluem:\n",
      "\n",
      "1. **DemocratizaÃ§Ã£o da tecnologia de IA**: A proliferaÃ§Ã£o de LLMs de cÃ³digo aberto estÃ¡ democratizando o acesso Ã  tecnologia de ponta de processamento de linguagem, permitindo que pesquisadores, desenvolvedores e amadores experimentem, personalizem e implantem soluÃ§Ãµes de IA com um investimento inicial mÃ­nimo.\n",
      "\n",
      "2. **IntegraÃ§Ã£o do LLM Ã s ferramentas de desenvolvimento de software e de escritÃ³rio**: A integraÃ§Ã£o do LLM Ã s ferramentas de desenvolvimento de software e de escritÃ³rio estÃ¡ transformando a eficiÃªncia e a capacidade das empresas, com exemplos como o Microsoft 365 Copilot e o Google Workspace.\n",
      "\n",
      "3. **DiversificaÃ§Ã£o de arquiteturas de LLM**: Os LLMs estÃ£o progredindo alÃ©m da simples previsÃ£o de texto e se tornando aplicativos sofisticados em vÃ¡rios domÃ­nios, arquiteturas e modalidades, incluindo LLMs baseados em redes neurais recorrentes (RNNs) e transformers.\n",
      "\n",
      "4. **AvanÃ§o na regulamentaÃ§Ã£o da IA**: HÃ¡ uma preocupaÃ§Ã£o crescente com as consideraÃ§Ãµes e os desafios Ã©ticos apresentados pelo desenvolvimento e uso de LLMs, levando a um avanÃ§o na regulamentaÃ§Ã£o da IA e da IA generativa em todo o mundo.\n",
      "\n",
      "Essas tendÃªncias podem estar relacionadas ao conceito de LangChain, mas nÃ£o hÃ¡ informaÃ§Ãµes especÃ­ficas sobre o tema no texto fornecido.\n",
      "\u001b[0m> Running step 00b0f793-f3b5-47cb-9e84-943ed051f1ee. Step input: None\n",
      "\u001b[1;3;38;5;200mThought: O tutorial_engine forneceu informaÃ§Ãµes sobre tendÃªncias gerais relacionadas ao desenvolvimento e aplicaÃ§Ã£o de Large Language Models (LLMs) que podem estar relacionadas ao conceito de LangChain. Embora nÃ£o haja menÃ§Ã£o especÃ­fica Ã s principais tendÃªncias em LangChain, as tendÃªncias identificadas podem ser relevantes para o tema.\n",
      "Answer: Algumas das tendÃªncias gerais relacionadas ao desenvolvimento e aplicaÃ§Ã£o de Large Language Models (LLMs) que podem estar relacionadas ao conceito de LangChain incluem a democratizaÃ§Ã£o da tecnologia de IA, a integraÃ§Ã£o do LLM Ã s ferramentas de desenvolvimento de software e de escritÃ³rio, a diversificaÃ§Ã£o de arquiteturas de LLM e o avanÃ§o na regulamentaÃ§Ã£o da IA. Essas tendÃªncias podem ser relevantes para o tema de LangChain, mas nÃ£o hÃ¡ informaÃ§Ãµes especÃ­ficas sobre o tema no texto fornecido.\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "response = agent.chat(\n",
    "    \"Quais as principais tendÃªncias em LAngChain que eu deveria estudar?\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
