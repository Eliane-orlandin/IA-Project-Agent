{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/elianeorlandin/Documents/Desenvolvimento/IA-Project-Agent/llamaindex_agents/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import Settings\n",
    "from llama_index.llms.groq import Groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "llm = Groq(model=\"llama-3.3-70b-versatile\", api_key=os.environ.get(\"GROQ_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcular_imposto_renda(rendimento:float) -> str:\n",
    "    \"\"\" \n",
    "    Calcula o imposto de renda com base no rendimento anual.\n",
    "    Args:\n",
    "        rendimento (float): Rendimento anual do contribuinte.\n",
    "    Returns:\n",
    "        str: O valor do imposto devido com base no rendimento.\n",
    "    \"\"\"\n",
    "    if rendimento <= 2000:\n",
    "        return \"Isento de imposto de renda.\"\n",
    "    elif 2000 < rendimento <= 5000:\n",
    "        imposto = (rendimento - 2000) * 0.10\n",
    "        return f\"Imposto devido é de: R$ {imposto:.2f}, base em um rendimento de R$ {rendimento:.2f}.\"\n",
    "    elif 5000 < rendimento <= 10000:\n",
    "        imposto = (rendimento - 5000) * 0.15 + 300\n",
    "        return f\"Imposto devido é de: R$ {imposto:.2f}, base em um rendimento de R$ {rendimento:.2f}.\"\n",
    "    else:\n",
    "        imposto = (rendimento - 10000) * 0.20 + 1050\n",
    "        return f\"Imposto devido é de: R$ {imposto:.2f}, base em um rendimento de R$ {rendimento:.2f}.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convertendo Função em Ferramenta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import FunctionTool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ferramenta_imposto_renda = FunctionTool.from_defaults(\n",
    "    fn=calcular_imposto_renda,\n",
    "    name=\"Calcular Imposto de Renda\",\n",
    "    description=(\n",
    "    \"Calcula o imposto de renda com base no rendimento anual do contribuinte.\"\n",
    "    \"Argumento: redimento (float).\"\n",
    "    \"Retorna o valor do imposto devido de acordo com faixas de rendimento.\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent import FunctionCallingAgentWorker "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_worker_imposto = FunctionCallingAgentWorker.from_tools(\n",
    "    tools=[ferramenta_imposto_renda],\n",
    "    verbose=True,\n",
    "    allow_parallel_tool_calls=True,\n",
    "    llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent import AgentRunner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_imposto= AgentRunner(agent_worker_imposto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: \n",
      "    Qual é o imposto de renda devido por uma pessoa com rendimento anual de R$ 7500?\n",
      "\n",
      "=== Calling Function ===\n",
      "Calling function: Calcular Imposto de Renda with args: {\"rendimento\": 7500}\n",
      "=== Function Output ===\n",
      "Imposto devido é de: R$ 675.00, base em um rendimento de R$ 7500.00.\n",
      "=== LLM Response ===\n",
      "O imposto de renda devido por uma pessoa com rendimento anual de R$ 7500 é de R$ 675,00.\n"
     ]
    }
   ],
   "source": [
    "response = agent_imposto.chat(\"\"\"\n",
    "    Qual é o imposto de renda devido por uma pessoa com rendimento anual de R$ 7500?\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv \n",
    "\n",
    "def consulta_artigos(titulo: str) -> str:\n",
    "    \"\"\" Consulta artigos na base de dados arXiv e retorna resultados formatados.\"\"\"\n",
    "    busca = arxiv.Search(\n",
    "        query=titulo,\n",
    "        max_results=5,\n",
    "        sort_by=arxiv.SortCriterion.Relevance\n",
    "    )\n",
    "\n",
    "    resultados = [\n",
    "        f\"Título: {artigo.title}\\n\"\n",
    "        f\"Categoria: {artigo.primary_category}\\n\"\n",
    "        f\"Link: {artigo.entry_id}\\n\"\n",
    "        for artigo in busca.results()\n",
    "    ]\n",
    "\n",
    "    return \"\\n\\n\".join(resultados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "consulta_artigos_tool = FunctionTool.from_defaults(fn=consulta_artigos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
    "    [ferramenta_imposto_renda, consulta_artigos_tool],\n",
    "    verbose=True,\n",
    "    allow_parallel_tool_calls=False,\n",
    "    llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Me retorne artigos sobre LangChain na eeducação\n",
      "=== Calling Function ===\n",
      "Calling function: consulta_artigos with args: {\"titulo\": \"LangChain na educa\\u00e7\\u00e3o\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5z/bv195b9d0f11m9c66rw80j3r0000gp/T/ipykernel_10203/3605877752.py:15: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead\n",
      "  for artigo in busca.results()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Function Output ===\n",
      "Título: Development and Testing of Retrieval Augmented Generation in Large Language Models -- A Case Study Report\n",
      "Categoria: cs.CL\n",
      "Link: http://arxiv.org/abs/2402.01733v1\n",
      "\n",
      "\n",
      "Título: From Prompt Injections to SQL Injection Attacks: How Protected is Your LLM-Integrated Web Application?\n",
      "Categoria: cs.CR\n",
      "Link: http://arxiv.org/abs/2308.01990v4\n",
      "\n",
      "\n",
      "Título: Automating Customer Service using LangChain: Building custom open-source GPT Chatbot for organizations\n",
      "Categoria: cs.CL\n",
      "Link: http://arxiv.org/abs/2310.05421v1\n",
      "\n",
      "\n",
      "Título: Poisoned LangChain: Jailbreak LLMs by LangChain\n",
      "Categoria: cs.CL\n",
      "Link: http://arxiv.org/abs/2406.18122v1\n",
      "\n",
      "\n",
      "Título: Breast Ultrasound Report Generation using LangChain\n",
      "Categoria: eess.IV\n",
      "Link: http://arxiv.org/abs/2312.03013v1\n",
      "\n",
      "=== Calling Function ===\n",
      "Calling function: consulta_artigos with args: {\"titulo\": \"LangChain e educa\\u00e7\\u00e3o\"}\n",
      "=== Function Output ===\n",
      "Título: From Prompt Injections to SQL Injection Attacks: How Protected is Your LLM-Integrated Web Application?\n",
      "Categoria: cs.CR\n",
      "Link: http://arxiv.org/abs/2308.01990v4\n",
      "\n",
      "\n",
      "Título: Automating Customer Service using LangChain: Building custom open-source GPT Chatbot for organizations\n",
      "Categoria: cs.CL\n",
      "Link: http://arxiv.org/abs/2310.05421v1\n",
      "\n",
      "\n",
      "Título: Poisoned LangChain: Jailbreak LLMs by LangChain\n",
      "Categoria: cs.CL\n",
      "Link: http://arxiv.org/abs/2406.18122v1\n",
      "\n",
      "\n",
      "Título: Breast Ultrasound Report Generation using LangChain\n",
      "Categoria: eess.IV\n",
      "Link: http://arxiv.org/abs/2312.03013v1\n",
      "\n",
      "\n",
      "Título: Revolutionizing Mental Health Care through LangChain: A Journey with a Large Language Model\n",
      "Categoria: cs.HC\n",
      "Link: http://arxiv.org/abs/2403.05568v1\n",
      "\n",
      "=== Calling Function ===\n",
      "Calling function: consulta_artigos with args: {\"titulo\": \"LangChain aplicado \\u00e0 educa\\u00e7\\u00e3o\"}\n",
      "=== Function Output ===\n",
      "Título: From Prompt Injections to SQL Injection Attacks: How Protected is Your LLM-Integrated Web Application?\n",
      "Categoria: cs.CR\n",
      "Link: http://arxiv.org/abs/2308.01990v4\n",
      "\n",
      "\n",
      "Título: Método de elementos finitos aplicado a las ecuaciones de Stokes y de Advección-Difusión\n",
      "Categoria: math.NA\n",
      "Link: http://arxiv.org/abs/1401.7619v1\n",
      "\n",
      "\n",
      "Título: Método de Monte Carlo aplicado ao Cálculo Fracionário\n",
      "Categoria: math.NA\n",
      "Link: http://arxiv.org/abs/2110.08129v1\n",
      "\n",
      "\n",
      "Título: Automating Customer Service using LangChain: Building custom open-source GPT Chatbot for organizations\n",
      "Categoria: cs.CL\n",
      "Link: http://arxiv.org/abs/2310.05421v1\n",
      "\n",
      "\n",
      "Título: Poisoned LangChain: Jailbreak LLMs by LangChain\n",
      "Categoria: cs.CL\n",
      "Link: http://arxiv.org/abs/2406.18122v1\n",
      "\n",
      "=== LLM Response ===\n",
      "Infelizmente, não foi possível encontrar artigos sobre LangChain aplicado à educação. No entanto, você pode tentar procurar em outras bases de dados ou realizar uma busca mais específica para encontrar resultados mais relevantes. Além disso, é importante notar que a pesquisa sobre LangChain e sua aplicação em diferentes áreas, incluindo a educação, é um campo em constante evolução, e novos estudos e artigos podem ser publicados a qualquer momento.\n"
     ]
    }
   ],
   "source": [
    "agent = AgentRunner(agent_worker)\n",
    "response = agent.chat(\"Me retorne artigos sobre LangChain na eeducação\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usando o Tavily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tavily_key = os.environ.get(\"TAVILY_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.tools.tavily_research import TavilyToolSpec\n",
    "\n",
    "tavily_tool = TavilyToolSpec(\n",
    "    api_key=tavily_key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search\n"
     ]
    }
   ],
   "source": [
    "tavily_tool_list = tavily_tool.to_tool_list()\n",
    "for tool in tavily_tool_list:\n",
    "    print(tool.metadata.name)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id_='29cb1e12-0732-4fcc-ba58-101ad8f4f339', embedding=None, metadata={'url': 'https://community.revelo.com.br/faca-perguntas-ao-seu-pdf-usando-langchain-llama-2-e-python/'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Durante 2023, a Meta anunciou o LLaMA 2 (de agora em diante vou chamá-lo simplesmente de Llama 2), um LLM de código aberto que é a evolução de seu modelo anterior (LLaMA 1), que pode ser usado para criar aplicações para fins comerciais. Para carregar o modelo Llama 2, como aconteceu com o carregamento de PDF e Pinecode, LangChain também nos fornece uma interface (que fácil!). Isto não significa que devolva dois parágrafos ou duas palavras, mas tem a ver com os textos que o modelo considera significativos para fornecer uma resposta: e um documento pode conter vários parágrafos. O que se vê neste artigo é apenas um vislumbre das capacidades do LangChain, já que possui muitas outras integrações e, além disso, permite que seja utilizado —através dos plugins— com outros modelos como ChatGPT.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tavily_tool.search(\"Me rteorne artigos científicos sobre LangChain\", max_results=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definição de ferramentas externas e integração com o agente  \n",
    "Criamos uma ferramenta utilizando a API do Tavily para realizar buscas sobre um determinado tópico.  \n",
    "Essa ferramenta será incorporada ao agente para permitir pesquisas contextuais.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import FunctionTool\n",
    "tavily_tool_function = FunctionTool.from_defaults(\n",
    "    fn=tavily_tool.search,\n",
    "    name=\"Tavily Search\",\n",
    "    description=(\n",
    "        \"Busca artigos com Tavily sobre um determinado tópico\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
    "    tools=[tavily_tool_function],\n",
    "    verbose=True,\n",
    "    allow_parallel_tool_calls=False,\n",
    "    llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = AgentRunner(agent_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Me retorne artigos sobre LLM e LangChain\n",
      "=== Calling Function ===\n",
      "Calling function: Tavily Search with args: {\"max_results\": 6, \"query\": \"LLM e LangChain\"}\n",
      "=== Function Output ===\n",
      "[Document(id_='8c34e89a-924c-43fc-bd19-0d50e14e8d26', embedding=None, metadata={'url': 'https://www.techtarget.com/searchenterpriseai/tip/How-to-use-LangChain-for-LLM-application-development'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='[LangChain](https://www.techtarget.com/searchenterpriseai/definition/LangChain), an open source framework for building AI applications, has become a de facto standard for working with LLMs and integrating [APIs](https://www.techtarget.com/searchapparchitecture/definition/application-program-interface-API). The tool serves as a critical intermediary, enabling a targeted LLM to interface with traditional software. [...] LangChain enables developers to integrate AI models with standard IT components such as software utilities, APIs and databases. Within LangChain, developers use a combination of prompts, tools and chains to manage LLMs and create desired AI interactions.\\n\\n### Prompts\\n\\nAll processes in LangChain revolve around [prompts](https://www.techtarget.com/searchenterpriseai/definition/AI-prompt), which initiate the tasks that AI applications rely on. [...] Using LangChain with Spark and Kafka\\n------------------------------------\\n\\nLangChain excels at managing LLM workflows and integrating language models with APIs, tools and software utilities. But AI applications often require more than just model orchestration.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='0fc8abcf-f41b-4ba2-bc70-61fb6a41950a', embedding=None, metadata={'url': 'https://vitiya99.medium.com/building-scalable-llm-applications-with-langchain-and-vector-databases-5541c0ebee0e'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Large Language Models (LLMs) are transforming how we interact with information and build intelligent applications. However, effectively leveraging their power for real-world scenarios often requires extending their capabilities beyond simple prompt-response interactions. This is where LangChain and vector databases come into play. LangChain provides a streamlined framework for developing LLM-powered applications, while vector databases facilitate efficient semantic search, enabling LLMs to [...] LangChain simplifies the complexities of integrating LLMs into applications by providing abstractions for common tasks like prompt management, chain execution, and memory management. Vector databases, such as Pinecone, Weaviate, and Faiss, store embeddings (vector representations of data) generated by models like Sentence Transformers or OpenAI’s embeddings API. This allows for similarity search, enabling the retrieval of contextually relevant information based on semantic meaning rather than [...] keyword matching. The combination of LangChain and vector databases empowers developers to build sophisticated LLM applications that can access, process, and reason over large amounts of unstructured data.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='c00ed611-1b03-4cf4-8dc8-77a9dd241cee', embedding=None, metadata={'url': 'https://scalexi.medium.com/understanding-the-differences-between-llm-chains-and-llm-agent-executors-in-langchain-3f3cf402442f'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=\"**LangChain** has emerged as a robust framework for building applications powered by large language models (LLMs). Two fundamental concepts within LangChain are **LLM Chains** and **LLM Agent Executors**, both of which leverage tools to enhance the capabilities of LLMs. While they may seem similar at first glance, understanding their differences is crucial for developers aiming to harness LangChain's full potential. [...] Both **LLM Chains** and **LLM Agent Executors** offer powerful ways to structure and execute tasks using LangChain, but they are designed for different use cases. Understanding the key differences is essential for choosing the right approach.\", path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='88498561-b8f8-4e79-b22d-aca3e7dd2392', embedding=None, metadata={'url': 'https://python.langchain.com/docs/integrations/llms/'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Unless you are specifically using more advanced prompting techniques, you are probably looking for [this page instead](https://python.langchain.com/docs/integrations/chat/).\\n\\n[LLMs](https://python.langchain.com/docs/concepts/text_llms/) are language models that take a string as input and return a string as output.\\n\\ninfo [...] [![Image 2: Open on GitHub](https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github&logoColor=white)](https://github.com/langchain-ai/langchain/blob/master/docs/docs/integrations/llms/index.mdx)\\n\\nLLMs\\n====\\n\\ncaution\\n\\nYou are currently on a page documenting the use of [text completion models](https://python.langchain.com/docs/concepts/text_llms/). Many of the latest and most popular models are [chat completion models](https://python.langchain.com/docs/concepts/chat_models/). [...] *   [Kinetica](https://python.langchain.com/docs/integrations/chat/kinetica/)\\n        *   [Konko](https://python.langchain.com/docs/integrations/chat/konko/)\\n        *   [LiteLLM](https://python.langchain.com/docs/integrations/chat/litellm/)\\n        *   [Llama 2 Chat](https://python.langchain.com/docs/integrations/chat/llama2_chat/)\\n        *   [Llama API](https://python.langchain.com/docs/integrations/chat/llama_api/)', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='999aa9f9-ca32-4830-bc52-7044d316cbce', embedding=None, metadata={'url': 'https://python.langchain.com/docs/tutorials/llm_chain/'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=\"In this quickstart we'll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it's just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!\\n\\nAfter reading this tutorial, you'll have a high level overview of: [...] Many of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with [LangSmith](https://smith.langchain.com/).\\n\\nAfter you sign up at the link above, make sure to set your environment variables to start logging traces: [...] [![Image 1: 🦜️🔗 LangChain](https://python.langchain.com/img/brand/wordmark.png)](https://python.langchain.com/)[Integrations](https://python.langchain.com/docs/integrations/providers/)[API Reference](https://python.langchain.com/api_reference/)\\n\\n[More](https://python.langchain.com/docs/tutorials/llm_chain/#)\", path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='d90d9fb5-b2d6-4d4e-96ec-7e303ab941fa', embedding=None, metadata={'url': 'https://vstorm.co/the-power-of-langchain-in-llm-based-applications/'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='LangChain, a framework specifically designed for Large Language Model (LLM) applications, has emerged as a major tool in enhancing the capabilities of natural', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}')]\n",
      "=== LLM Response ===\n",
      "Os artigos sobre LLM e LangChain abordam a integração de modelos de linguagem grandes (LLMs) com a framework LangChain para desenvolver aplicações de inteligência artificial (IA) mais avançadas. A LangChain fornece uma estrutura para gerenciar workflows de LLMs e integrá-los com APIs, ferramentas e utilitários de software, permitindo que os desenvolvedores criem aplicações de IA mais complexas e escaláveis.\n",
      "\n",
      "Os artigos destacam a importância da LangChain para simplificar a integração de LLMs em aplicações, fornecendo abstrações para tarefas comuns como gerenciamento de prompts, execução de chains e gerenciamento de memória. Além disso, a combinação da LangChain com bancos de dados de vetores permite a realização de buscas semânticas eficientes, permitindo que os LLMs acessem e processem grandes quantidades de dados não estruturados.\n",
      "\n",
      "Os artigos também discutem a diferença entre LLM Chains e LLM Agent Executors, dois conceitos fundamentais na LangChain, e como eles podem ser usados para estruturar e executar tarefas em aplicações de IA. Além disso, os artigos fornecem exemplos de como a LangChain pode ser usada para desenvolver aplicações de IA, como tradução de texto e busca semântica.\n",
      "\n",
      "Em resumo, os artigos sobre LLM e LangChain destacam a importância da integração de modelos de linguagem grandes com a framework LangChain para desenvolver aplicações de inteligência artificial mais avançadas e escaláveis. A LangChain fornece uma estrutura para gerenciar workflows de LLMs e integrá-los com APIs, ferramentas e utilitários de software, permitindo que os desenvolvedores criem aplicações de IA mais complexas e eficientes.\n"
     ]
    }
   ],
   "source": [
    "response = agent.chat(\"Me retorne artigos sobre LLM e LangChain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Os artigos sobre LLM e LangChain abordam a integração de modelos de linguagem grandes (LLMs) com a framework LangChain para desenvolver aplicações de inteligência artificial (IA) mais avançadas. A LangChain fornece uma estrutura para gerenciar workflows de LLMs e integrá-los com APIs, ferramentas e utilitários de software, permitindo que os desenvolvedores criem aplicações de IA mais complexas e escaláveis.\n",
      "\n",
      "Os artigos destacam a importância da LangChain para simplificar a integração de LLMs em aplicações, fornecendo abstrações para tarefas comuns como gerenciamento de prompts, execução de chains e gerenciamento de memória. Além disso, a combinação da LangChain com bancos de dados de vetores permite a realização de buscas semânticas eficientes, permitindo que os LLMs acessem e processem grandes quantidades de dados não estruturados.\n",
      "\n",
      "Os artigos também discutem a diferença entre LLM Chains e LLM Agent Executors, dois conceitos fundamentais na LangChain, e como eles podem ser usados para estruturar e executar tarefas em aplicações de IA. Além disso, os artigos fornecem exemplos de como a LangChain pode ser usada para desenvolver aplicações de IA, como tradução de texto e busca semântica.\n",
      "\n",
      "Em resumo, os artigos sobre LLM e LangChain destacam a importância da integração de modelos de linguagem grandes com a framework LangChain para desenvolver aplicações de inteligência artificial mais avançadas e escaláveis. A LangChain fornece uma estrutura para gerenciar workflows de LLMs e integrá-los com APIs, ferramentas e utilitários de software, permitindo que os desenvolvedores criem aplicações de IA mais complexas e eficientes.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
